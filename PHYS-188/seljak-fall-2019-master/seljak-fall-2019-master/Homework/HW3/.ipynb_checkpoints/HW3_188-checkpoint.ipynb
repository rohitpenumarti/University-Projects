{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "## <em>Intro to Data Analysis, Dimensionality Reduction, and Clustering</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw3_188.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Fitting Gaussian Contours to a 2D Data\n",
    "\n",
    "Gaussian distribution function plays a central role in statistics and is the most ubiquitous distribution in physics. It often provides a good approximation to the true probability density function (pdf) even in cases where its application is not strictly correct. <br><br>\n",
    "In this problem, suppose that you have measured 1000 pairs of values $(x_1, y_1), ... , (x_{1000}, y_{1000})$ of two variables $x, y$. You saved these measurements to a .dat file (\"Problem1_data.dat\"). Plot their 1-dimensional pdf's and determine how well Gaussian pdf can approximate them. Compute the mean, variance, median, mode, 68% and 95% confidence intervals, and correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> 1. Plot 1-dimensional pdf for $x$. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a given 2D data\n",
    "data = np.loadtxt(\"Problem1_data.dat\")\n",
    "x = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot a normalized histogram (Hint - https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.hist.html)\n",
    "# Try to have 25 elements per each bin\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Calculate mean, variance, and median of $x$. First, do it \"by hand\" without using any in-built functions. Then, check your answers using in-built functions from numpy. </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating \"by hand\"\n",
    "mean_x = ...\n",
    "variance_x = ...\n",
    "\n",
    "median_x = ...\n",
    "\n",
    "print(\"For x, mean = \", mean_x, \", variance = \", variance_x, \", and median = \", median_x)\n",
    "\n",
    "# Using in-built functions from numpy\n",
    "mean_x = ...\n",
    "variance_x = ...\n",
    "median_x = ...\n",
    "\n",
    "print(\"For x, mean = \", mean_x, \", variance = \", variance_x, \", and median = \", median_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. Smoothly interpolate the discrete probability density from Part 1. Then, find the mode and symmetric 68%, 95% confidence intervals. (Suggestion - Read https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html and https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)</i></span><br><br>\n",
    "(Hint - For 68% confidence interval, find the range where 68% of the given sample occurs. We assume that such interval is symmetrically placed around the mean. <br>\n",
    "In other words, find $a$ such taht\n",
    "$$ 0.68 = \\int_{\\mu-a}^{\\mu+a} P(x) $$\n",
    "where $P(x)$ is $x$'s pdf, and $\\mu$ is the mean.<br>\n",
    "One way to find $a$ is to define a cumulative distribution function (cdf) $G(x)$ and find $a$ such that $G(\\mu+a)-G(\\mu-a) = .68$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The following skeleton code is only a suggestion.\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "p, xvar = np.histogram( ... , normed = True)\n",
    "# Find x values at the center of each bin (call it xvar)\n",
    "xvar = xvar[:-1] + (xvar[1] - xvar[0])/2\n",
    "\n",
    "# Find the interpolated function f\n",
    "f = UnivariateSpline(xvar, p, s=n)\n",
    "# Change the amount of smoothing\n",
    "f.set_smoothing_factor(1.e-8)\n",
    "\n",
    "# Plot both histogram and interpolated function\n",
    "plt.hist( ... , normed = True, rwidth = 0.8)\n",
    "plt.plot( ... , '--', label = \"Interpolated\")\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the mode\n",
    "mode_x = ...\n",
    "\n",
    "print(\"For x, mode = \", mode_x)\n",
    "\n",
    "# Find 68% and 95% confidence intervals\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the distribution is Gaussian, 68% and 95% confidence interval corresponds to $\\mu \\pm 1\\sigma$ and $\\mu \\pm 2\\sigma$.  </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Assuming Gaussian distribution, 68% confidence interval is\", mean_x, \"±\", np.sqrt(variance_x), \n",
    "     \", and 95% interval is\", mean_x, \"±\", 2*np.sqrt(variance_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the Gaussian distribution is a reasonable approximation in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Plot Gaussian distribution with the mean and variance from Part 1 on top of probability density histogram. Make sure to label each plot. </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Gaussian distribution\n",
    "def gaussian(x, mu, sigma):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "...\n",
    "# Plot Gaussian distribution on top\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"> <i> 5. Repeat part 1-4 for $y$.  </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find mean, variance and median\n",
    "...\n",
    "# Plot histogram (discrete pdf) and interpolate it\n",
    "...\n",
    "# Find mode and 68%, 95% confidence interval\n",
    "...\n",
    "# Plot Gaussian fit on top of the histogram\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 6. Make a 2-d scatter plot with Gaussian contours (ellipses). Then, compute the covariance ($C_{xy}$) of $x$ and $y$ as well as the correlation coefficient $\\rho = \\frac{C_{xy}}{\\sigma_{x}\\sigma_{y}}$.  </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the covariance and correlation coefficient\n",
    "cov = ...\n",
    "\n",
    "print(\"The covariance between x and y is\", cov)\n",
    "\n",
    "print(\"The correlation coefficient of x and y is\", ...)\n",
    "\n",
    "# Make a 2-d scatter plot with Gaussian contours\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "# Create coordinate matrices from coordinate vectors.\n",
    "gridx = np.linspace(9000, 11000, 100)\n",
    "gridy = np.linspace(-1500, 1500, 100)\n",
    "X, Y = np.meshgrid(gridx, gridy)\n",
    "\n",
    "# Create bivariate Gaussian distribution for equal shape X, Y (https://matplotlib.org/api/mlab_api.html)\n",
    "Z = mlab.bivariate_normal(X, Y, np.sqrt(variance_x), np.sqrt(variance_y), mean_x, mean_y, cov)\n",
    "\n",
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "# Scatter plot\n",
    "plt.scatter(x, y, marker = 'x')\n",
    "# Gaussian contour plots\n",
    "plt.contour(X, Y, Z)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above contour plot is a bird eye view of the 3-d mesh plot; these are ellipses of equal probability. The coloring represents the intensity. Yellow central ellipse is the region of highest probability; the peak of 2-d Gaussian distribution is at the center of this ellipse. As we move away from the peak, the probability lowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Central Limit Theorem\n",
    "\n",
    "Plot the binomial distribution $P(N_A, N)$ for different values of $N$ and plot the Gaussian with mean and variance for the binomial. Similarly, plot the Poisson distribution with the mean varying from 1 to 10. See if both binomial and Poisson approach Gaussian as the mean/$N$ increases.<br><br>\n",
    "(Reference - Kardar p. 41) For the binomial distribution, consider a random variable with two outcomes $A$ and $B$ of relative probabilities $p_A$ and $p_B = 1 - p_A$. The probability that in $N$ trials the event $A$ occurs exactly $N_A$ times is given by the binomial distribution:\n",
    "$$ p_N(N_A) = \\binom{N}{N_A} p_A^{N_A}(1-p_A)^{N-N_A}. $$\n",
    "<br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot the binomial distribution $P(N_A, N)$ for $N = 5, 20, 40, 100, 300$ and plot the Gaussian with mean and variance for the binomial. Let $p_A = 0.5$ and $0.1$. Make sure to label each plot.  </i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages for the bionomial coefficient\n",
    "from scipy.special import binom\n",
    "\n",
    "# Define the probability for the binomial distribution\n",
    "def pdf_binom(p_A, N, N_A):\n",
    "    return ...\n",
    "\n",
    "# Define Gaussian distribution\n",
    "def gaussian(x, mu, sigma):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = [5, 20, 40, 100, 300]\n",
    "\n",
    "# Make plot\n",
    "\n",
    "# For p_A = 0.1\n",
    "N_A = np.linspace(0, 40, 1000)\n",
    "plt.figure(figsize= (10, 8))\n",
    "p_A = 0.1\n",
    "...\n",
    "plt.show()\n",
    "\n",
    "# For p_A = 0.5\n",
    "N_A = np.linspace(0, 70, 1000)\n",
    "plt.figure(figsize= (10, 8))\n",
    "p_A = 0.5\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we find that the binomial distribution is approximately normal (with mean $Np_A$ and variance $Np_A(1-p_A)$) as $N \\rightarrow \\infty$, by the central limit theorem. The proof of this theorem can be carried out using Stirling's approximation:\n",
    "$$ N! \\approx N^N e^{-N}\\sqrt{2\\pi N} $$\n",
    "<br>\n",
    "<span style=\"color:blue\"><i> 2. Plot the above Stirling's formula approximation (i.e. Compare $N!$ with Stirling's approximation. Compute the residual: (actual-estimate)/actual.) </i></span><br>\n",
    "(Hint: $\\Gamma(n+1) = n!$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "Nvals = np.linspace(1, 40, 1000)\n",
    "\n",
    "actual = ...\n",
    "estimate = ...\n",
    "\n",
    "plt.plot(Nvals, (actual-estimate)/actual)\n",
    "plt.xlabel('$N$')\n",
    "plt.ylabel('residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that residual $\\rightarrow 0$ as $N \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the Poisson distribution (Kardar p. 42):\n",
    "$$ P(\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n",
    "where $k$ is the number of occurrences. Its mean and variance are $\\lambda$.<br><br>\n",
    "<span style=\"color:blue\"> <i> 3. Plot $P(\\lambda)$ as a function of $k$ for $\\lambda = 1, 3, 5, 10, 20$ and plot the Gaussian with mean and variance for the Poisson. Make sure to label. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Poisson distribution\n",
    "...\n",
    "\n",
    "# Make plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. What happens as the mean/$N$ increases? </i></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> Answer: </i></span><br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3 - Fitting Data to a Straight Line (Linear Regression)\n",
    "\n",
    "(Reference - NR 15.2) We fit a set of 50 data points $(x_i, y_i)$ to a straight-line model $y(x) = a + bx$. The uncertainty $\\sigma_i$ associated with each measurement $y_i$ is known, and we assume that the $x_i$'s are known exactly. To measure how well the model agrees with the data, we use the chi-square merti function: <br>\n",
    "$$ \\chi^2(a,b) = \\sum_{i=0}^{N-1} \\big( \\frac{y_i-a-bx_i}{\\sigma_i} \\big)^2. $$\n",
    "<br>\n",
    "Make a scatter plot of data (including uncertainties) and find the best-fit line. Compute the errors on the two parameters $a$ and $b$ and plot lines where the two are changed by $\\pm 1\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot data (make sure to include error bars). (Hint - https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.errorbar.html) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a given 2D data\n",
    "data = np.loadtxt(\"Problem3_data.dat\")\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "sig_y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "# Scatter plot\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NR p. 781) We should minimize the above chi-square function to determine $a$ and $b$. At its minimum, derivatives of $\\chi^2$ with respect to $a, b$ vanish:\n",
    "$$ \\frac{\\partial{\\chi^2}}{\\partial{a}} = -2 \\sum \\frac{y_i - a - bx_i}{\\sigma_i^2} = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1) $$\n",
    "$$ \\frac{\\partial{\\chi^2}}{\\partial{b}} = -2 \\sum \\frac{x_i(y_i - a - bx_i)}{\\sigma_i^2} = 0   \\ \\ \\ \\ \\ \\ \\ \\ \\ (2) $$\n",
    "<br>\n",
    "These conditions can be rewritten in a convenient form if we define the following sums:\n",
    "$$ S = \\sum \\frac{1}{\\sigma_i^2},\\ S_x = \\sum \\frac{x_i}{\\sigma_i^2},\\ S_y = \\sum \\frac{y_i}\n",
    "{\\sigma_i^2} $$\n",
    "$$ S_{xx} = \\sum \\frac{x_i^2}{\\sigma_i^2},\\ S_{xy} = \\sum \\frac{x_iy_i}{\\sigma_i^2} $$\n",
    "<br> With these, we can rewrite (1), (2) as:\n",
    "$$ a*S + b*S_x = S_y $$\n",
    "$$ a*S_x + b*S_{xx} = S_{xy} $$\n",
    "<br> The solution to these is calculated as:\n",
    "$$ \\Delta = SS_{xx} - (S_x)^2 $$ <br>\n",
    "$$ a = \\frac{S_{xx}S_y - S_xS_{xy}}{\\Delta} $$\n",
    "$$ b = \\frac{SS_{xy} - S_xS_y}{\\Delta} $$\n",
    "<br><span style=\"color:blue\"><i> 2. Find parameters $a, b$ which minimize the chi-square function and plot the best-fit line on top of the data. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = ...\n",
    "Sx = ...\n",
    "Sy = ...\n",
    "Sxx = ...\n",
    "Sxy = ...\n",
    "Delta = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ...\n",
    "b = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must estimate the probable uncertainties in the estimates of $a$ and $b$, since obviously the measurement errors in the data must introduce some uncertainty in the determination of those parameters. If the data are independent, then each contributes its own bit of uncertainty to the parameters. Consideration of propagation of errors show that the variance $\\sigma_f^2$ in the value of any function will be \n",
    "$$ \\sigma_f^2 = \\sum \\sigma_i^2 (\\frac{\\partial f}{\\partial y_i})^2 $$\n",
    "<br> For the straight line, the derivatives of $a$ and $b$ with respect to $y_i$ can be directly evaluated from teh solution:\n",
    "$$ \\frac{\\partial a}{\\partial y_i} = \\frac{S_{xx}-S_x x_i}{\\sigma_i^2 \\Delta} $$\n",
    "$$ \\frac{\\partial b}{\\partial y_i} = \\frac{S x_i-S_x}{\\sigma_i^2 \\Delta} $$\n",
    "<br> Summing over the points, we get\n",
    "$$ \\sigma_a^2 = S_{xx}/\\Delta $$\n",
    "$$ \\sigma_b^2 = S/\\Delta $$\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. Compute the errors ($\\sigma_a, \\sigma_b$) on the two parameters $a, b$ and plot lines where the two are changed by $\\pm 1\\sigma$.</i></span><br>\n",
    "(Hint - Try to plot the 1$\\sigma$ confidece band as in http://astropython.blogspot.com/2011/12/. You can use plt.fill_between to shade the region between plots.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate sigma_a, sigma_b\n",
    "\n",
    "sigma_a = ...\n",
    "sigma_b = ...\n",
    "\n",
    "print('We estimate that a =', a ,\"±\", sigma_a, \"and b =\", b, \"±\", sigma_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 7))\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4 - UMAP + clustering\n",
    "\n",
    "Yann LeCun and collaborators collected and processed  70,000  handwritten digits to produce what became known as the most widely used database in ML, called MNIST. Each handwritten digit comes in a square image, divided into a  28×28  pixel grid. Every pixel can take on  256  nuances of the gray color, interpolating between white and black, and hence each the data point assumes any value in the set  {0,1,…,255}. There are  10  categories in the problem, corresponding to the ten digits.\n",
    "\n",
    "Ever since, the MNIST problem has become an important standard for benchmarking the performance of more sophisticated Machine Learning models. Often times, there are contests for finding a new constellation of hyperparameters and/or model architecture which results in a better accuracy for correctly classifying the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "mnist = datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "# Load MNIST data\n",
    "X = mnist.data\n",
    "Y = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"$X$\" contains information about the given MNIST digits. We have a 28x28 pixel grid, so each image is a vector of length 784; we have 70,000 images (digits), so $X$ is a 70,000x784 matrix. \"$Y$\" is a label (0-9; the category to which each image belongs) vector of length 70,000.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Do the following:\n",
    "\n",
    "(1) Randomly shuffle data (i.e. randomize the order)\n",
    "\n",
    "  (Note: The label $Y_1$ corresponds to a vector $X_{1j}$, and even after shuffling, $Y_1$ should still correspond to $X_{1j}$.)\n",
    "  \n",
    "  \n",
    "(2) Select only 20% of the data. (due to memory issues; if your code keeps crashing, you can go down to 10%.)\n",
    "\n",
    "\n",
    "(3) Split data into training and test samples using train_test_split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Set train_size = 0.8. (80% of $X$ is our training samples.) Print the dimension of training and test samples. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# shuffle data\n",
    "...\n",
    "\n",
    "# pick training and test data sets \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(...)\n",
    "\n",
    "# Select only 20% of data\n",
    "...\n",
    "\n",
    "print( np.shape(X_train), np.shape(Y_train) )\n",
    "print( np.shape(X_test), np.shape(Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Choose any five images and show what they look like. Also, print the corresponding label to each image. </i></span> <br>\n",
    "\n",
    "Hint: each image is a vector of length 784. So reshape it into a 28x28 matrix.\n",
    "\n",
    "&nbsp; **X_0 = X_train[0]** <br>\n",
    "&nbsp; **X_0 = X_0.reshape((28, 28))**\n",
    "  \n",
    "Then, make a plot using imshow\n",
    "\n",
    "&nbsp; **plt.imshow(X_0, cmap=plt.cm.gray)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exercises are taken from the tutorial here: https://umap-learn.readthedocs.io/en/latest/index.html. Please go through them if you want to learn more about the umap package.\n",
    "\n",
    "**UMAP** (Uniform Manifold Approximation and Projection for Dimension Reduction) is a general purpose manifold learning and dimension reduction algorithm. It is designed to be compatible with scikit-learn, making use of the same API and able to be added to sklearn pipelines. If you are already familiar with sklearn you should be able to use UMAP as a drop in replacement for t-SNE and other dimension reduction classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While UMAP can be used for standard unsupervised dimension reduction the algorithm offers significant flexibility allowing it to be extended to perform other tasks, including making use of categorical label information to do supervised dimension reduction, and even metric learning.\n",
    "\n",
    "Let's use labels (\"$Y$\") to do supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go about coercing UMAP to make use of target labels? If you are familiar with the sklearn API you’ll know that the fit() method (here, you can think of \"fitting\" as \"training.\") takes a target parameter y that specifies supervised target information (for example when training a supervised classification model). We can simply pass the UMAP model that target data when fitting and it will make use of it to perform supervised dimension reduction!\n",
    "\n",
    "First, define your UMAP model:\n",
    "\n",
    "&nbsp; **model = umap.UMAP()**\n",
    "\n",
    "Then, using \"fit_transform,\" fit your training sample into an embedded space, with the target array (\"Y\") for supervised dimension reduction specified.\n",
    "\n",
    "&nbsp; **embedding_train = model.fit_transform(training X data, training Y label)**\n",
    "\n",
    "You can use this trained model to transform new data - which is \"test data\" in this case.\n",
    "\n",
    "&nbsp; **embedding_test = model.transform(test X data)**\n",
    "\n",
    "By default, data has reduced down to 2 dimensions. (i.e. embedding_test will have dimensions = (# of test data, 2) - it is an array with test samples, but with 2 feature columns. Each row of the array is a 2-dimensional representation of the corresponding digit.) Thus we can plot the embedding as a standard scatterplot and color by the target array (since it applies to the transformed data which is in the same order as the original).\n",
    "\n",
    "&nbsp; **plt.scatter(embedding[:, 0], embedding[:, 1], ...)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. Do the supervised learning and create scatterplots for both training and test data. Color data points according to the corresponding target array $Y$. Make sure to label all 10 classes. Are they cleanly separated?</i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP has several hyperparameters that can have a significant impact on the resulting embedding: n_neighbors, min_dist, n_components, and metric. (https://umap-learn.readthedocs.io/en/latest/parameters.html)\n",
    "\n",
    "Each of these parameters has a distinct effect, and we will look at n_neighbors and n_components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_neighbors** controls how UMAP balances local versus global structure in the data. It does this by constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data. This means that low values of n_neighbors will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point when estimating the manifold structure of the data, losing fine detail structure for the sake of getting the broader of the data.\n",
    "\n",
    "We can see that in practice by fitting our dataset with UMAP using a range of n_neighbors values. The default value of n_neighbors for UMAP (as used above) is 15.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Choose n_neighbors=2,200. What happens as we increase n_neighbors? </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a value of n_neighbors=2 we see that UMAP merely glues together small chains, but due to the narrow/local view, fails to see how those connect together. It also leaves many different components (and even singleton points). This represents the fact that from a fine detail point of view the data is very disconnected and scattered throughout the space.\n",
    "\n",
    "As n_neighbors is increased UMAP manages to see more of the overall structure of the data, gluing more components together, and better coverying the broader structure of the data. By the stage of n_neighbors=20 we have a fairly good overall view of the data showing how the various colors interelate to each other over the whole dataset.\n",
    "\n",
    "As n_neighbors increases further more and more focus in placed on the overall structure of the data. This results in, with n_neighbors=200 a plot where the overall structure is well captured, but at the loss of some of the finer local sturcture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is standard for many scikit-learn dimension reduction algorithms UMAP provides a **n_components** parameter option that allows the user to determine the dimensionality of the reduced dimension space we will be embedding the data into. Unlike some other visualisation algorithms such as t-SNE UMAP scales well in embedding dimension, so you can use it for more than just visualisation in 2- or 3-dimensions.\n",
    "\n",
    "For the purposes of this demonstration (so that we can see the effects of the parameter) we will only be looking at 1-dimensional and 3-dimensional embeddings, which we have some hope of visualizing.\n",
    "\n",
    "First of all we will set n_components to 1, forcing UMAP to embed the data in a line. For visualisation purposes we will randomly distribute the data on the y-axis to provide some separation between points.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 5. Choose n_components=1,3. Observe the scatterplots. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses. Fortunately UMAP makes this possible, albeit more slowly than some other transformers that allow this.\n",
    "\n",
    "To make use of UMAP as a data transformer we first need to fit the model with the training data. In this case we simply hand it the training data and it will learn an appropriate (two dimensional by default) embedding.\n",
    "\n",
    "&nbsp; **model = umap.UMAP()**\n",
    "\n",
    "&nbsp; **embedding_train_unsupervised = model.fit_transform(training X data)**\n",
    "\n",
    "&nbsp; **embedding_test_unsupervised = model.transform(test X data)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 6. Do the unsupervised learning and create scatterplots for both training and test data. Color data points according to the corresponding target array $Y$. Make sure to label all 10 classes. Compared to Part 3, how did they perform? Also, has the test data been embedded into 2 dimensions in exactly the locations we should expect (by class) given the embedding of the training data? </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP can be used as an effective preprocessing step to boost the performance of density based clustering. This is somewhat controversial, and should be attempted with care. For a good discussion of some of the issues involved in this please see the various answers in this stackoverflow thread (https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne) on clustering the results of t-SNE. Many of the points of concern raised there are salient for clustering the results of UMAP. The most notable is that UMAP, like t-SNE, does not completely preserve density. UMAP, like t-SNE, can also create tears in clusters that are not actually present, resulting in a finer clustering than is necessarily present in the data. Despite these concerns there are still valid reasons to use UMAP as a preprocessing step for clustering. As with any clustering approach one will want to do some exploration and evaluation of the clusters that come out to try to validate them if possible.\n",
    "\n",
    "With all of that said, let’s work through an example to demonstrate the difficulties that can face clustering approaches and how UMAP can provide a powerful tool to help overcome them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 7. Let's do unsupervised learning and identify distinct classes with the help of UMAP: </i></span> <br>\n",
    "\n",
    "(1) First, reduce the data to 2-dimensions using UMAP. Do unsupervised learning on the training data. Choose \"n_neighbors=30,min_dist=0.0,random_state=42\"\n",
    "\n",
    "For clustering, try HDBSCAN, which we believe to be among the most advanced density based tehcniques. \n",
    "\n",
    "(2) Create the HDBSCAN model (You can change the hyperparameter setting):\n",
    "\n",
    "&nbsp; **HDBSCAN_model = hdbscan.HDBSCAN(min_samples=10,min_cluster_size=100)**\n",
    "\n",
    "(3) Take the embedding you created in (1) and get predicted classes using \"fit_predict\"\n",
    "\n",
    "&nbsp; **HDBSCAN_labels = HDBSCAN_model.fit_predict(embedding)**\n",
    "\n",
    "(4) Predicted label = -1 means that HDBSCAN refused to cluster that points and classify the as “noise.” Make the scatterplot in 2d using umap embedding and color points according to the cluster membership. Color noise as gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: hdbscan will be installed on datahub before Tuesday (9/17)!\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 8. Evaluate the adjusted Rand score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) and adjusted mutual information (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) for this clustering as compared with the true labels. </i></span> <br>\n",
    "\n",
    "(Note: Changing the hyperparameters of UMAP/HDBSCAN model and increasing the total number of sample, you can improve those scores.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adjusted Rand score: \", adjusted_rand_score(...))\n",
    "print(\"adjusted mutual information: \", adjusted_mutual_info_score(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
