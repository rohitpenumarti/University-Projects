{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 - Part 2\n",
    "\n",
    "## <em> Optimization, Markov chain Monte Carlo, Bayesfast </em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> Write your partner's name here (if you have one). </i></span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('project1_p2_288.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Planck analysis continued - Nonlinear (Quadratic) Model\n",
    "\n",
    "\n",
    "In Project 1 - part 1, we assumed a simple linear model of the theory CMB power spectrum with 6 cosmological parameters, $[\\Omega_b h^2, \\Omega_c h^2, H_0, \\tau, A_s, n_s]$, and found the best-fit model using linear algebra and Gauss-Newton method, but optimization was rather trivial with the given linear model. In Part 2, we add quadratic terms to our model of the CMB power spectrum with 2 additional parameters: $\\Omega_k$ (curvature density parameter) and $y_{\\\\ cal}$ (Planck survey calibration parameter). We also replace $H_0$ with $100\\theta_{MC}$ (a measure of the sound horizon at last scattering), which can be converted into $H_0$.\n",
    "\n",
    "In summary, we take a <span style=\"color:blue\">quadratic</span> model of the CMB power spectrum with the following 8 paramters: <span style=\"color:blue\">$$[\\Omega_b h^2, \\Omega_c h^2, 100\\theta_{MC}, \\tau, \\Omega_K, {\\rm{ln}}(10^{10} A_s), n_s, y_{\\rm cal}]$$</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('/srv/app/venv/assignment-1910')\n",
    "from cosmofast.planck2018._simall import _simall_f, _simall_j, _simall_fj\n",
    "from cosmofast.planck2018._plik_lite_diag import _plik_lite_f, _plik_lite_j, _plik_lite_fj, _get_binned_cls\n",
    "from cosmofast.planck2018._commander import _commander_f, _commander_j, _commander_fj\n",
    "import dill\n",
    "dill.settings['recurse'] = True\n",
    "  \n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Planck measurements, we take 28 unbinned TT power spectrum for $2 \\leq \\ell < 30$, 28 unbinned EE power spectrum for $2 \\leq \\ell < 30$, and 215 binned TT power spectrum for $30 \\leq \\ell < 2500$ - total of 271 data points.\n",
    "\n",
    "Our quadratic model defined below (\"quadmodel\") can evaluate the power spectrum in the above 271 $\\ell$ bins, given 7 cosmological parameters ($\\Omega_b h^2, \\Omega_c h^2, 100\\theta_{MC}, \\tau, \\Omega_K, {\\rm{ln}}(10^{10} A_s), n_s$) - note that the calibration parameter is excluded.\n",
    "\n",
    "Suppose that our initial guess of the 8 model parameter is:\n",
    "$$ \\textbf{x0} = [\\Omega_b h^2, \\Omega_c h^2, 100\\theta_{MC}, \\tau, \\Omega_K, {\\rm{ln}}(10^{10} A_s), n_s, y_{\\rm cal}] = [ 0.022,  0.10 ,  1.  ,  0.08, 0., 3.  ,  0.958 ,  1. ].$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quadratic model for Planck 2018 Omega_K lilelihood\n",
    "# Note that we do linear extrapolation outside the sample range\n",
    "with open('/srv/app/venv/assignment-1910/data/den2.p', 'rb') as f:\n",
    "    quadmodel = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([ 0.022,  0.10 ,  1.  ,  0.08, 0., 3.  ,  0.958 ,  1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given our initial guess x0, we can evaluate model power spectra in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadmodel._module_list[0](x0[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that it outputs the power spectra in 271 $\\ell$ bins.\n",
    "\n",
    "Note that 215 high-$\\ell$ ($\\ell > 30)$ TT power spectrum from the model (the last 215 entries) is also diagonalized (multiplied by the cholesky of the likelihood covariance) so that the likelihood is simply -0.5*(data-model)^2.\n",
    "\n",
    "Now let's compare this diagonalized high-$\\ell$ TT power spectrum to the measured data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load measured data\n",
    "Y = np.loadtxt('data_Planck.dat')\n",
    "ell = Y[0,:]\n",
    "measured = Y[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"ell\" is the effective $\\ell$ values in 215 $\\ell$ bins, and \"measured\" is the corresponding measured TT power spectra.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Show how well the high-$\\ell$ TT model power spectra (given our initial guess x0) fits the data. i.e. Plot a \"scatterplot\" of the measured data (use '.' marker) and the high-$\\ell$ TT model power spectra (use a dotted line '--') in one figure. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try the <b>maximum a posteriori estimation (MAP)</b>. \n",
    "\n",
    "**quadmodel(x0)** outputs the log-posterior (logp) distribution. To get the MAP estimate, we should maximize logp (i.e. (data-model) is minimized). Then, we need graident and hessian of -logp.\n",
    "\n",
    "Given x0, we can evaluate the gradient of -logp:\n",
    "\n",
    "&nbsp; **-quadmodel.grad(x0)**\n",
    "\n",
    "The hessian of -logp given x0 is:\n",
    "\n",
    "&nbsp; **Hessian(lambda xx: -quadmodel(xx), 1e-4)(x0)**\n",
    "\n",
    "Note that \"-quadmodel.grad\" and \"Hessian(lambda xx: -quadmodel(xx), 1e-4)\" are functions, and you can evaluate them with different model parameter values.\n",
    "\n",
    "Now, let us use scipy.optimize.minimize (https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to get the MAP estimate of the model parameters: we can minimize -logp to get the MAP estimate. \n",
    "\n",
    "<span style=\"color:blue\"> <i> 2. Using the Newton conjugate gradient method ('Newton-CG'), find the MAP estimate of 8 model parameters with scipy.optimize.minimize. Print MAP estimates of the 8 model parameters. </i></span><br>\n",
    "\n",
    "Hint: you can do\n",
    "\n",
    "opt_NCG = minimize(**\"1.-logp function\"**, **2.initial guess (x0 in this case)**, method='Newton-CG', jac=**3.function for gradient of -logp**, hess=**4.function for gradient of -logp**, options={'return_all': True, 'disp': True})\n",
    "\n",
    "After optimization terminated successfully, you can retrieve the best-fit parameters with:\n",
    "\n",
    "&nbsp; **opt_NCG.x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from hessian import Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_NCG = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =  ['\\\\Omega_b h^2', '\\\\Omega_c h^2', '100\\\\theta_{MC}', '\\\\tau', \n",
    "           '\\Omega_K', '{\\\\rm{ln}}(10^{10} A_s)', 'n_s', 'y_{\\\\rm cal}']\n",
    "for i in range(len(labels)):\n",
    "    print(\"MAP value of %s = %.5f\" %(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"> <i> 3. Repeat Part 1 with the best-fit model you have found in Part 2. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cells to create an animation which shows how the model spectrum fits better with the measured data as we iterate in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots(figsize=(10, 7), edgecolor='black')\n",
    "rc('axes', linewidth=1.1)\n",
    "\n",
    "\n",
    "ax.set_xlim(( 0, 2500))\n",
    "ax.set_ylim((2, 76))\n",
    "ax.set_xticks([0, 500, 1000, 1500, 2000, 2500])\n",
    "ax.set_xticklabels([0, 500, 1000, 1500, 2000, 2500], fontsize = 14)\n",
    "\n",
    "ax.set_yticks([10, 20, 30, 40, 50, 60, 70])\n",
    "ax.set_yticklabels([10, 20, 30, 40, 50, 60, 70], fontsize = 14)\n",
    "\n",
    "ax.set_xlabel('$\\ell$', fontsize = 17)\n",
    "ax.set_ylabel('high-$\\ell$ CMB (TT) power spectrum \\n (binned and diagonalized)', fontsize = 17)\n",
    "\n",
    "\n",
    "ax.plot(ell, measured, '.', label = 'data')\n",
    "ax.set_facecolor('whitesmoke')\n",
    "ax.grid(linestyle='--', linewidth='0.5', color='grey')\n",
    "line, = ax.plot([], [], lw=2, label = 'model (fit)')\n",
    "\n",
    "#initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line, = ax.plot([], [], lw=2)\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    y = quadmodel._module_list[0](opt_NCG.allvecs[i][:7])[56:]\n",
    "    line.set_data(ell, y)\n",
    "    ax.text(200, 70, 'iteration = %d' %i, color='black',fontsize = 22,\n",
    "        bbox={'facecolor': 'white', 'alpha': 1.0, 'pad': 4.2, 'edgecolor': 'white'})\n",
    "    return (line,)\n",
    "\n",
    "ax.legend(fontsize = 16)\n",
    "\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=opt_NCG.nit, interval=500, blit=True, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Markov chain Monte Carlo is a general method based on drawing values of $\\theta$ from approximate distributions and then correcting those draws to better aproximate the target posterior distribution. The sampling is done sequentially, wtih the distribution of the sampled draws depending on the last value drawn - hence, the draws from a Markov chain. (p. 275, <i>Bayesian Data Analysis</i>, Andrew Gelman et al.) (Remember that a sequence $x_1, x_2, ...$ of random events is called a Markov chain if $x_{n+1}$ depends explicitly on $x_{n}$ only (and not explicitly on previous steps).) Here, we consider our 8 model paramters, so the \"chain\" in this case is a random walk through the parameter space.\n",
    "<br>\n",
    "![alt text](MCMC.png \"Title\")\n",
    "from https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "<br><br>\n",
    "As shown in the above figure, chains take time to converge to the target distribution, and you can determine the \"burn-in\" period, the number of sequences it takes to reach convergence.\n",
    "<br><br>\n",
    "In Part 4, we provide you MCMC chains from Planck. You can plot the chains in the parameter space and estimate the posterior distribution.\n",
    "<br><br>\n",
    "<i>References:</i><br>\n",
    "Bayesian Data Analysis, Andrew Gelman et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we provide you with 4 independent Planck MCMC chains. For each chain, we load the data for 8 model parameters, $[\\Omega_b h^2, \\Omega_c h^2, 100\\theta_{MC}, \\tau, \\Omega_K, {\\rm{ln}}(10^{10} A_s), n_s, y_{\\rm cal}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the official Planck chains\n",
    "x_mcmc = np.concatenate((\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_1.txt')[:,2:10][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_2.txt')[:,2:10][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_3.txt')[:,2:10][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_4.txt')[:,2:10][:3724]))\n",
    "p_mcmc = np.concatenate((\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_1.txt')[:,0][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_2.txt')[:,0][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_3.txt')[:,0][:3724],\n",
    "    np.loadtxt('/srv/app/venv/assignment-1910/data/base_omegak_plikHM_TT_lowl_lowE_4.txt')[:,0][:3724]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdist import plots, MCSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use MCSamples from **getdist** package (https://getdist.readthedocs.io/en/latest/mcsamples.html) to create a class of MCMC samples. \n",
    "\n",
    "&nbsp; **MCMC = MCSamples(samples=x_mcmc, weights=p_mcmc)**\n",
    "\n",
    "To get actual samples from this class, we can do:\n",
    "\n",
    "&nbsp; **MCMC_samples = MCMC.samples**\n",
    "\n",
    "We reshape this in the following way:\n",
    "\n",
    "&nbsp; **MCMC_samples = MCMC_samples.reshape(# of samples per chain, # of chains, # of parameters)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"x%s\"%i for i in range(8)]\n",
    "labels =  ['\\\\Omega_b h^2', '\\\\Omega_c h^2', '100\\\\theta_{MC}', '\\\\tau', \n",
    "           '\\Omega_K', '{\\\\rm{ln}}(10^{10} A_s)', 'n_s', 'y_{\\\\rm cal}']\n",
    "MCMC = MCSamples(samples=x_mcmc, weights=p_mcmc, names=names, labels=labels)\n",
    "MCMC_samples = MCMC.samples\n",
    "MCMC_samples = MCMC_samples.reshape(3724,4,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MCMC, we need to make sure that chains converge to the posterior distribution. One useful test for convergence is \"Gelman-Rubin statistic.\" For a given parameter, $\\theta$, the $R$ statistic compares the variance across chains with the variance within a chain. Intuitively, if the chains are random-walking in very different places, i.e. not sampling the same distribution, $R$ will be large.<br><br>\n",
    "In detail, given chains $J=1,\\ldots,m$, each of length $n$,<br>\n",
    "Let $B=\\frac{n}{m-1} \\sum_j \\left(\\bar{\\theta}_j - \\bar{\\theta}\\right)^2$, where $\\bar{\\theta_j}$ is the average $\\theta$ for chain $j$ and $\\bar{\\theta}$ is the global average. This is proportional to the variance of the individual-chain averages for $\\theta$.<br>\n",
    "Let $W=\\frac{1}{m}\\sum_j s_j^2$, where $s_j^2$ is the estimated variance of $\\theta$ within chain $j$. This is the average of the individual-chain variances for $\\theta$.<br>\n",
    "Let $V=\\frac{n-1}{n}W + \\frac{1}{n}B$. This is an estimate for the overall variance of $\\theta$.<br><br>\n",
    "Finally, $R=\\sqrt{\\frac{V}{W}}$.\n",
    "We'd like to see $R\\approx 1$ (e.g. $R < 1.1$ is often used). Note that this calculation can also be used to track convergence of combinations of parameters, or anything else derived from them. \n",
    "<br><br>\n",
    "Reference: https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 4. For all eight parameters, compute $R$ and determine if the condition $R < 1.1$ is satisfied.  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation of a sequence, as a function of lag, $k$, is defined thusly:\n",
    "$$\\rho_k = \\frac{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)\\left(\\theta_{i+k} - \\bar{\\theta}\\right)}{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)^2} = \\frac{\\mathrm{Cov}_i\\left(\\theta_i,\\theta_{i+k}\\right)}{\\mathrm{Var}(\\theta)}$$\n",
    "<br><br>\n",
    "The larger lag one needs to get a small autocorrelation, the less informative individual samples are.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 5. Using autocorrelation_plot from pandas (https://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-autocorrelation), plot the auto-correlation of six parameters and determine that it gets small for large lag. The given Planck MCMC chains are already heavily thinned, so you will not see much autocorrelation. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"> <i> 6. From the chain, obtain 1-d constraints on all 8 parameters. Print results. Also, show how far your MAP estimates are from the MCMC means ( Say (MAP estimate-MCMC mean)/MCMC standard deviation = 2. Then, your MAP estimate is 2 sigma away from the MCMC mean. )  </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot 1-d and 2-d constraints of our model parameters using \"getdist\" package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.getSubplotPlotter()\n",
    "g.triangle_plot([MCMC], filled=True, contour_args={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 7. Back to MAP. Plot 1-d and 2-d constraints from MAP. For this task, we should first evaluate the hessian of -logp at our MAP estimates and convert it to covariance. (See Part 2 for hints) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hess = ...\n",
    "cov = ...\n",
    "mean = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot MCMC and MAP results in a single figure, we should create samples from MAP. First, define a Gaussian distribution with mean and covariance defined above. Then, draw 5000 samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdist.gaussian_mixtures import GaussianND\n",
    "GNd=GaussianND(mean, cov, label='MAP')\n",
    "x_g = GNd.MCSamples(5000, names=names, labels=labels)\n",
    "MAP = MCSamples(samples=x_g.samples, names=names, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 8. Evaluate below cells to get results from MAP, MCMC, and bayesfast. </i></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare MCMC and MAP results: MAP gets the peak of the posterior most of the time, but the Laplace approximation fails to give the full posterior shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.getSubplotPlotter()\n",
    "g.settings.figure_legend_frame = False\n",
    "g.settings.legend_fontsize = 20\n",
    "g.triangle_plot([MCMC,MAP], filled=True, \n",
    "                colors=['green', 'red', 'royalblue'], legend_labels=['MCMC', 'MAP'], \n",
    "                legend_loc='upper right', line_args=[{'lw':2, 'color':'red'}, {'ls':'--', 'lw':2, 'color':'green'}],\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the method called \"bayesfast\" (developed by He and Seljak), which is a general fast Bayesian posterior method using the optimization-based posterior inference method called EL$_2$O (from Seljak and Yu, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesfast.samplers.pymc3.nuts import NUTS\n",
    "import bayesfast.utils.warnings as bfwarnings\n",
    "import warnings\n",
    "\n",
    "# Below is the temporary solution for printing the sampling progress\n",
    "# We plan to rewrite it with e.g. tqdm in the future\n",
    "warnings.showwarning = bfwarnings.showwarning_chain()\n",
    "warnings.formatwarning = bfwarnings.formatwarning_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, EL$_2$O is combined with quadratic surrogate HMC, and this allows a fast evaluation of the posterior. We start from our MAP estimates and draw 2500 HMC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = NUTS(max_treedepth=8, \n",
    "            logp_and_grad=(lambda xx: quadmodel.logp_and_grad(xx, False, False)),\n",
    "            x_0=quadmodel.from_original(opt_NCG.x), \n",
    "            random_state=np.random.RandomState(0), \n",
    "            metric=None, step_size=1., target_accept=0.9)\n",
    "sample_pre_transform = nuts.run(2500, 500)\n",
    "sample_post_transform = quadmodel.to_original(np.array(sample_pre_transform.samples[501:]))\n",
    "bayesfast = MCSamples(samples=sample_post_transform, names=names, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the bayesfast posterior is almost equivalent to the MCMC posterior, and it succeeds in finding the non-Gaussian feature of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.getSubplotPlotter()\n",
    "g.settings.figure_legend_frame = False\n",
    "g.settings.legend_fontsize = 20\n",
    "g.triangle_plot([MAP, MCMC, bayesfast], filled=True, \n",
    "                colors=['green', 'red', 'royalblue'], legend_labels=['MAP+Laplace', 'MCMC', 'bayesfast'], \n",
    "                legend_loc='upper right', line_args=[{'ls':'--', 'lw':2, 'color':'green'}, \n",
    "                                                     {'lw':2, 'color':'red'}, {'lw':2, 'color':'royalblue'}],\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Supernova Cosmology Project\n",
    "\n",
    "In this homework, we use a compilation of supernovae data to show that the expansion of the universe is accelerating, and hence it contains dark energy. This is the Nobel prize winning research in 2011 (https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/), and Saul Perlmutter, a professor of physics at Berkeley, shared a prize in 2011 for this discovery.\n",
    "<br><br>\n",
    "\"The expansion history of the universe can be determined quite easily, using as a “standard candle” any distinguishable class of astronomical objects of known intrinsic brightness that can be identified over a wide distance range. As the light from such beacons travels to Earth through an expanding universe, the cosmic expansion stretches not only the distances between galaxy clusters, but also the very wavelengths of the photons en route. By the time the light reaches us, the spectral wavelength $\\lambda$ has thus been redshifted by precisely the same incremental factor $z = \\Delta \\lambda/\\lambda$ by which the cosmos has been stretched in the time interval since the light left its source. The recorded redshift and brightness of each such object thus provide a measurement of the total integrated expansion of the universe since the time the light was emitted. A collection of such measurements, over a sufficient range of distances, would yield an entire historical record of the universe’s expansion.\" (Saul Perlmutter, http://supernova.lbl.gov/PhysicsTodayArticle.pdf).\n",
    "<br><br>\n",
    "Supernovae emerge as extremely promising candidates for measuring the cosmic expansion. Type I Supernovae arises from the collapse of white dwarf stars when the Chandrasekhar limit is reached. Such nuclear chain reaction occurs in the same way and at the same mass, the brightness of these supernovae are always the same. The relationship between the apparent brightness and distance of supernovae depend on the contents and curvature of the universe.\n",
    "<br><br>\n",
    "We can infer the \"luminosity distance\" $D_L$ from measuring the inferred brightness of a supernova of luminosity $L$. Assuming a naive Euclidean approach, if the supernova is observed to have flux $F$, then the area over which the flux is distributed is a sphere radius $D_L$, and hence <br><br>\n",
    "$$F = \\frac{L}{4\\pi D_L^2}.$$\n",
    "<br>\n",
    "In Big Bang cosmology, $D_L$ is given by:\n",
    "<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} $$\n",
    "<br>\n",
    "where $a$ is the scale factor ($\\frac{\\lambda_0}{\\lambda} = 1 + z = \\frac{a_0}{a}$, and the quantity with the subscript 0 means the value at present. Note that $a_0 = 1, z_0 = 0$.), and $\\chi$ is the comoving distance, the distance between two objects as would be measured instantaneously today. For a photon, $cdt = a(t)d\\chi$, so $\\chi(t) = c\\int_t^{t_0} \\frac{dt'}{a(t')}$. We can write this in terms of a Hubble factor ($H(t) = \\frac{1}{a}\\frac{da}{dt}$), which tells you the expansion rate: $\\chi(a) = c\\int_a^1 \\frac{da'}{a'^2H(a')} = c\\int_0^z \\frac{dz'}{H(z')}$. (change of variable using $a = \\frac{1}{1+z}$.)\n",
    "<br><br>\n",
    "Using the Friedmann equation (which basically solves Einstein's equations for a homogenous and isotropic universe), we can write $H^2$ in terms of the mass density $\\rho$ of the components in the universe: $H^2(z) = H_0^2[\\Omega_m(1+z)^3 + (1-\\Omega_m)(1+z)^2].$ <br><br>\n",
    "$\\Omega$ is the density parameter; it is the ratio of the observed density of matter and energy in the universe ($\\rho$) to the critical density $\\rho_c$ at which the universe would halt is expansion. So $\\Omega_0$ (again, the subscript 0 means the value at the present) is the total mass and energy density of the universe today, and consequently $\\Omega_0 = \\Omega_{m}$ (matter density parameter today; remember we obtained the best-fit value of this parameter in Project 1?) = $\\Omega_{\\mathrm{baryonoic\\ matter}}$ + $\\Omega_{\\mathrm{dark\\ matter}}$. If $\\Omega_0 < 1$, the universe will continue to expand forever. If $\\Omega_0 > 1$, the expansion will stop eventually and the universe will start to recollapse. If $\\Omega_0 = 1$, then the universe is flat and contains enough matter to halt the expansion but not enough to recollapse it. So it will continue expanding, but gradually slowing down all the time, finally running out of steam only in the infinite future. Even including dark matter in this calculation, cosmologists found that all the matters in the universe only amounts to about a quarter of the required critical mass, suggesting a continuously expanding universe with deceleration. Then, using all this, we can write the luminosity distance in terms of the density parameters: <br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.\n",
    "<br><br>\n",
    "Fluxes can be expressed in magnitudes $m$, where $m = -2.5\\cdot\\mathrm{log}_{10}F$ + const. The distance modulus is $\\mu = m - M$ ($M$ is the absolute magnitude, the value of $m$ if the supernova is at a distance 10pc. Then, we have:\n",
    "<br><br>\n",
    "$$ \\mu = 25 + 5\\cdot \\mathrm{log}_{10}\\Big(D_L\\ [in\\ the\\ unit\\ of \\ Mpc]\\Big)$$\n",
    "<br><br>\n",
    "In this assignment, we use the SCP Union2.1 Supernova (SN) Ia compilation. (http://supernova.lbl.gov/union/)\n",
    "<br><br>\n",
    "First, load the measured data: $z$ (redshift), $\\mu$ (distance modulus), $\\sigma(\\mu)$ (error on distance modulus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> 1. Plot the measured distance modulus as a function of redshift with errorbars. Then, assume three different scenarios: $\\Omega_m = 0, 0.3, 1.$  </i></span> <br><br>\n",
    "Remember:\n",
    "$$ D_L = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}$$ <br>\n",
    "$$ \\mu = 25 + 5\\cdot \\mathrm{log}_{10}(D_L)$$ <br><br>\n",
    "<span style=\"color:blue\"><i> Now, plot three curves of $\\mu$ as a function of $z$ for $\\Omega_m = 0, 0.3, 1$ on top of the measured data (Calculate $D_L$ using quad. For now, assume $h = 0.7$.) How do they fit? </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,14))\n",
    "\n",
    "...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the measured data do not fit well to all three scenarios. \"The high-redshift supernovae are fainter than would be expected even for an empty cosmos (corresponding to $\\Omega_m = 0$).\" So what's wrong? \n",
    "<br><br>\n",
    "\"If these data are correct, the obvious implication is that the simplest cosmological model must be too simple. The next simplest model might be one that Einstein entertained for a time. Believing the universe to be static, he tentatively introduced into the equations of general relativity an expansionary term he called the “cosmological constant” ($\\Lambda$) that would compete against gravitational collapse. After Hubble’s discovery of the cosmic expansion, Einstein famously rejected $\\Lambda$ as his “greatest blunder.” In later years, $\\Lambda$ came to be identified with the zero-point vacuum energy of all quantum fields. It turns out that invoking a cosmological constant allows us to fit the supernova data quite well.\" (Saul Perlmutter, https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/)\n",
    "<br><br>\n",
    "So in short, the data indicates that faint supernovae are further away from the earth than had been theoretically expected. The expansion rate of the universe is increasing indeed. It seems that some mysterious material (which we call \"dark energy\") is causing such antigravity effects. The cosmological constant, $\\Lambda$, the value of the energy density of the vacuum of space is widely accepted as a leading candidate of dark energy. \n",
    "<br><br>\n",
    "Now let us add a general form of dark energy to our model.\n",
    "<br><br>\n",
    "$$H^2(z) = H_0^2[\\Omega_m(1+z)^3 + \\Omega_{DE}(1+z)^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z)^2].$$ <br> $w$ is the dark energy equation of state, which is the ratio of its pressure to its energy density. $w = -1$ for the cosmological constant $\\Lambda$. <br><br>\n",
    "$\\Omega_0 = \\Omega_{m}$ (matter density parameter today) + $\\Omega_{DE}$ (dark energy density parameter today), and \n",
    "<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + \\Omega_{DE}(1+z')^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z')^2]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + \\Omega_{DE}(1+z')^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z')^2]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Now assume three different scenarios: ($\\Omega_m = 0.3, \\Omega_{DE} = 0$), ($\\Omega_m = 0, \\Omega_{DE} = 1, w = -1$), and ($\\Omega_m = 0.3, \\Omega_{DE} = 0.7, w = -1$). Again, plot three curves of $\\mu$ as a function of $z$ on top of data (assume $h = 0.7$) </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,14))\n",
    "\n",
    "...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You basically reproduced the below figure!\n",
    "![alt text](SN.png \"Title\")\n",
    "You should see that $\\Omega_m = 0.3$ and $\\Omega_m = 0.7$ fits the data best. In combination with the CMB data, this shows that about 70% of the total energy density is vacuum energy and 30% is mass.\n",
    "***\n",
    "Now, with measurements of the distance modulus $\\mu$, use Bayesian analysis to estimate the cosmological parameters.\n",
    "<br><br>\n",
    "let us assume that the universe is flat (which is a fair assumption since the CMB measurements indicate that the universe has no large-scale curvature). $\\Omega_0 = \\Omega_m + \\Omega_{DE} = 1$. Then, we do not need to worry about the curvature term:<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}\\ [unit\\ of\\ Mpc] $$<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.<br><br>\n",
    "Assuming that errors are Gaussian (can be justified by averaging over large numbers of SN; central limit theorem), we calculate the likelihood $L$ as: <br><br>\n",
    "$$ L \\propto \\mathrm{exp}\\Big( -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} \\Big) $$\n",
    "<br>\n",
    "where $z_i, \\mu_i, \\sigma(\\mu_i)$ are from the measurements, and we compute $\\mu_{model}$ as a function of $z, \\Omega_m, w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, write an MCMC code using the <b>Metropolis algorithm</b>. \n",
    "\n",
    "Now, assume a more general form of dark energy. (Do not fix $w$ to -1; add $w$ as a parameter.)\n",
    "\n",
    "In the flat universe, <br><br>\n",
    "\n",
    "$$ D_L = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$. Here, we fix $h = 0.7$.<br><br>\n",
    "We calculate the likelihood $L$ as: <br><br>\n",
    "$$ \\mathrm{ln}(L) \\approx -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} = -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{\\Delta \\mu_i^2}{\\sigma(\\mu_i)^2} $$\n",
    "<br>\n",
    "where $$ \\mu_{i,\\ model}(z_i, \\Omega_m, w) = 25 + 5\\cdot \\mathrm{log}_{10}(D_{L,\\ i})$$<br>\n",
    "$$ D_{L,\\ i} = \\frac{2997.92458}{0.7} (1+z_i)\\int_0^{z_i} \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}} $$\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 6. Run the MCMC code to estimate $w$ and $\\Omega_m$. Plot 1-d posterior of $w$ and $\\Omega_m$ as well as 2-d posterior (i.e. plot the chain in two-dimensional parameter space. Make sure that the chain has converged (you can change nsamples, nburn).  </i></span><br><br>\n",
    "\n",
    "Hint:\n",
    "\n",
    "Set the length of MCMC chains to be 15,000 (or even more if you think that the chain has not yet converged.) In the end, you should throw away the first 20% of the chain as burn-in. (20% is an arbitrary number. You can plot the chain and estimate the burn-in period.)\n",
    "\n",
    "Then, set the random initial point in the parameter space $(w, \\Omega_m)$: let $w$ be negative and $\\Omega_m$ be positive and draw a random number using np.random.uniform(). Set initial likelihood to low value (e.g. -1.e100) so that next point is accepted.\n",
    "\n",
    "Now, draw a new sample starting from this random initial point. Here we assume that the proposal distribution is Gaussian with arbitrary width: in this problem, we assume that $\\sigma = 0.01$ (This determines how far you propose jumps.) for distributions for both $w, \\Omega_m$.\n",
    "\n",
    "For example, say that you start with $(w, \\Omega_m)$ = $(-0.3, 0.7)$. Then, draw a new sample of $w$ from a Gaussian with $\\mu = -0.3, \\sigma = 0.01$ and a new sample of $\\Omega_m$ from a Gaussian with $\\mu = -0.7, \\sigma = 0.01$.\n",
    "\n",
    "Now, evaluate the log likelihood value of this new point.\n",
    "\n",
    "If the value has gone up, accept the point.\n",
    "\n",
    "Otherwise, accept it with probability given by ratio of likelihoods:\n",
    "Draw a random number from a uniform distribution between 0 and 1 ( $\\alpha$ = np.random.uniform() ). If the ratio $ln(\\frac{L_{new}}{L_{old}})$ is greater than $ln(\\alpha)$ (i.e. $\\frac{L_{new}}{L_{old}} > \\alpha$), then accept it. Otherwise, reject it and stay at your old point.\n",
    "\n",
    "Repeat this 15,000 times (the length of chain) and plot the distributions of $(w, \\Omega_m)$.\n",
    "\n",
    "See the undergrad version for more hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood function:\n",
    "\n",
    "def lnL(Omegam, w):\n",
    "\n",
    "    # Treat unphysical regions by setting likelihood to (almost) zero:    \n",
    "    if(Omegam<=0 or w>=0):\n",
    "        lnL = -1.e100\n",
    "    else:\n",
    "            \n",
    "    # Compute difference with theory mu at redshifts of the SN, for trial Omegam\n",
    "    # Compute ln(likelihood) assuming gaussian errors\n",
    "        ...\n",
    "        \n",
    "    return lnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw new proposed samples from a proposal distribution, centred on old values\n",
    "# Accept or reject, and colour points according to ln(likelihood):\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
