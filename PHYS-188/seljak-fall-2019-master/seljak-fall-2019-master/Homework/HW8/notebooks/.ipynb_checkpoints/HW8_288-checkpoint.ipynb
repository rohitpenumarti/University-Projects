{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5221b8hT_4PL"
   },
   "source": [
    "## Homework 8 - part 1\n",
    "\n",
    "## <em> VI, EL$_2$O, Generative Models, Multimodal Posteriors, and Gaussian Processes</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw8_288.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "D-Fe5G8m1FTC",
    "outputId": "e48ffc61-2e4e-4327-a63b-324e5fee0cc6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%pylab inline\n",
    "import pickle\n",
    "import corner\n",
    "import time\n",
    "PROJECT_PATH = os.path.abspath('../')\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_hub as hub\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "from tensorflow.contrib.distributions import softplus_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Image reconstruction from noisy and incomplete images\n",
    "\n",
    "This exercise is based on https://arxiv.org/pdf/1910.10046.pdf:\n",
    "\n",
    "Data reconstruction from corrupted data lies at the heart of Bayesian inverse problems. There are many ways in which data can be corrupted, ranging from missing data (or masking) to noise and blurring. In many cases the reconstructed data, e.g. an image, is high dimensional. Data reconstruction commonly faces two challenges: 1) For most data it is difficult to find a prior that optimally represents our knowledge of the uncorrupted data. Instead, priors are often chosen to impose certain regularity conditions (e.g. maximum entropy, smoothness). 2) While it is usually tractable to find the maximum of the posterior, i.e. the most probable underlying realization, a full uncertainty quantification of the reconstruction is usually prohibitively expensive.\n",
    "\n",
    "We address these challenges with generative models, which provide a mapping from points in a typically lower dimensional latent space to points in the high dimensional data space. As an example of such a model we will be using a Variational AutoEncoder (VAE). \n",
    "\n",
    "VAEs are designed to model the distribution $p_{\\phi}(\\textbf{x})$ of high-dimensional input data, $\\textbf{x}$, by introducing a mapping $p_{\\phi}(\\textbf{x}|\\textbf{z})$ to a lower dimensional latent representation, $\\textbf{z}$. The latent space variables are enforced to follow a given prior distribution, $p(\\textbf{z})$, which is typically chosen to be a standard normal distribution.\n",
    "\n",
    "Given a generative model, the posterior of the latent variables for a given data realization can be modeled with Bayes rule\n",
    "$$ p_{\\phi}(\\textbf{z}|\\textbf{x}) \\propto p_{\\phi}(\\textbf{x}|\\textbf{z})p(\\textbf{z}) $$\n",
    "\n",
    "This formulation allows to address both aforementioned problems: 1) The prior distribution, p(z), reflects the distribution of the training data. 2) The representation of the posterior in the lower dimensional latent space enables tractable posterior analysis. In particular it allows to examine and fit the posterior distribution and draw samples from it. The samples can then be visualized in data space by forward modeling with the generative model.\n",
    "\n",
    "In this exercise, consider examples of data corruption on the MNIST dataset. First, load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to load mnist dataset\n",
    "import gzip, zipfile, tarfile\n",
    "import os, shutil, re, string, urllib, fnmatch\n",
    "import pickle as pkl\n",
    "\n",
    "def _download_mnist_realval(dataset):\n",
    "    \"\"\"\n",
    "    Download the MNIST dataset if it is not present.\n",
    "    :return: The train, test and validation set.\n",
    "    \"\"\"\n",
    "    origin = (\n",
    "        'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "    )\n",
    "    print('Downloading data from %s' % origin)\n",
    "    urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "def _get_datafolder_path():\n",
    "    full_path = os.path.abspath('.')\n",
    "    path = full_path +'/data'\n",
    "    return path\n",
    "\n",
    "def load_mnist_realval(\n",
    "        dataset=_get_datafolder_path()+'/mnist_real/mnist.pkl.gz'):\n",
    "    '''\n",
    "    Loads the real valued MNIST dataset\n",
    "    :param dataset: path to dataset file\n",
    "    :return: None\n",
    "    '''\n",
    "    if not os.path.isfile(dataset):\n",
    "        datasetfolder = os.path.dirname(dataset)\n",
    "        if not os.path.exists(datasetfolder):\n",
    "            os.makedirs(datasetfolder)\n",
    "        _download_mnist_realval(dataset)\n",
    "\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pkl.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    x_train, targets_train = train_set[0], train_set[1]\n",
    "    x_valid, targets_valid = valid_set[0], valid_set[1]\n",
    "    x_test, targets_test = test_set[0], test_set[1]\n",
    "    return x_train, targets_train, x_valid, targets_valid, x_test, targets_test\n",
    "  \n",
    "x_train, targets_train, x_valid, targets_valid, x_test, targets_test = load_mnist_realval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's create a corrupted image. Here, we mask half of the image and add noise with to the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim    = 28*28 # data dimensionality\n",
    "data_size   = 1     # number of corrupted input data, always 1\n",
    "sigma_n     = 0.1   # noise level used in the likelihood, determined during VAE training or measured\n",
    "hidden_size = 10    # latent space dimensionality of the VAE\n",
    "sample_size = 512   # number of samples drawn in each step of the ELBO minimization (for stochastic VI only)\n",
    "n_channels  = 1     # mnist is b/w -> 1 channel\n",
    "seed        = 777   # random seed\n",
    "\n",
    "### choose one of the following to run different examples\n",
    "\n",
    "# settings for reconstruction with noise and mask\n",
    "corr_type   = 'noise+mask'\n",
    "num_mnist   = 6\n",
    "label       = 'masknoise05'\n",
    "noise_level = 0.5\n",
    "num_comp    = 3\n",
    "\n",
    "PROJECT_PATH = os.path.abspath('../')\n",
    "plot_path        = os.path.join(PROJECT_PATH,'plots/')\n",
    "\n",
    "# Saving plots\n",
    "plot_path = os.path.join(plot_path,'%s/'%label)\n",
    "\n",
    "if not os.path.isdir(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "    \n",
    "def plot_image(image, save=True, directory='./plots/',filename='plotted_image', title='image',vmin=None,vmax=None, mask=None):\n",
    "    \"\"\"\n",
    "    plots and saves a single image of mnist format\n",
    "    \"\"\"\n",
    "\n",
    "    if np.any(mask==None):\n",
    "        mask=np.ones_like(image)\n",
    "    mask = np.reshape(mask,(28,28))\n",
    "    plt.figure()\n",
    "    #plt.title(title)\n",
    "    plt.imshow((image).reshape((28,28))*mask,cmap='gray',vmin=vmin, vmax=vmax)\n",
    "    plt.axis('off')\n",
    "    #plt.colorbar()\n",
    "    if save: \n",
    "        plt.savefig(directory+filename+'.pdf',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_custom_noise(shape, signal_dependent=False, signal =None, sigma_low=0.07, sigma_high=0.22, threshold=0.02 ):\n",
    "    \"\"\"\n",
    "    adds the model error and the noise level in the data to obtain the total sigma that is used in the likelihood (option to make the reconstruction error signal dependent is not used) \n",
    "    \"\"\"\n",
    "    sigma = np.ones(shape)*sigma_n\n",
    "\n",
    "    if signal_dependent: \n",
    "        for ii in range(data_size):\n",
    "            sigma[ii][np.where(signal[ii]<=threshold)]= sigma_low\n",
    "            sigma[ii][np.where(signal[ii]>threshold)]= sigma_high\n",
    "\n",
    "    data_noise = np.ones_like(sigma)*noise_level\n",
    "\n",
    "    sigma = np.sqrt(sigma**2+data_noise**2)\n",
    "\n",
    "    return sigma\n",
    "  \n",
    "\n",
    "def make_corrupted_data(x_true, corr_type='mask'):\n",
    "    \"\"\"\n",
    "    creates the corrupted image according to chosen corruption type\n",
    "    \"\"\"\n",
    "\n",
    "    mask = np.ones((28,28))\n",
    "\n",
    "    if corr_type=='mask':\n",
    "\n",
    "        minx = 10\n",
    "        maxx = 24\n",
    "\n",
    "        mask[0:28,minx:maxx]=0.\n",
    "        mask = mask.reshape((28*28))\n",
    "\n",
    "        corr_data = x_true*[mask]\n",
    "\n",
    "    elif corr_type=='sparse mask':\n",
    "\n",
    "        mask    = np.ones(data_dim, dtype=int)\n",
    "        percent = 95\n",
    "        np.random.seed(seed+2)\n",
    "        indices = np.random.choice(np.arange(data_dim), replace=False,size=int(percent/100.*data_dim))\n",
    "        print('precentage masked:', len(indices)/data_dim)\n",
    "        mask[indices] =0 \n",
    "\n",
    "        corr_data = x_true*[mask]\n",
    "\n",
    "    elif corr_type=='noise':\n",
    "\n",
    "        np.random.seed(seed+2)\n",
    "        noise = np.random.randn(data_dim*data_size)*noise_level\n",
    "\n",
    "        corr_data = x_true+noise\n",
    "\n",
    "    elif corr_type=='noise+mask':\n",
    "\n",
    "        np.random.seed(seed+2)\n",
    "        noise = np.random.randn(data_dim*data_size)*noise_level\n",
    "\n",
    "        minx = 14\n",
    "        maxx = 28\n",
    "\n",
    "        mask[0:28,minx:maxx]=0.\n",
    "        mask = mask.reshape((28*28))\n",
    "\n",
    "        corr_data = x_true+noise\n",
    "        corr_data = corr_data*[mask]\n",
    "\n",
    "    elif corr_type=='none':\n",
    "\n",
    "        corr_data = x_true\n",
    "\n",
    "    corr_data = np.expand_dims(corr_data,-1)\n",
    "\n",
    "    mask = mask.flatten()\n",
    "\n",
    "    return corr_data, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-defined function \"plot_image,\" you can easily plot an image of MNIST datasets. Below we plot corrupted data and the underlying truth. Note that plots are saved in HW8/plots/ in your datahub directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create underlying truth\n",
    "truth = x_test[num_mnist:num_mnist+data_size]\n",
    "# create corrupted data\n",
    "data, custom_mask = make_corrupted_data(truth, corr_type=corr_type)\n",
    "# noise\n",
    "noise = get_custom_noise(data.shape, signal_dependent=False, signal=truth)\n",
    "\n",
    "# Make plot\n",
    "print('Truth:')\n",
    "plot_image(truth, directory=plot_path, filename='truth_%s'%label, title='truth')\n",
    "print('Data:')\n",
    "plot_image(data, directory=plot_path, filename='input_data_%s'%label, title='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already trained a VAE on the uncorrupted training set with both encoder, $f_{\\psi}$, and decoder, $g_{\\phi}$, parameterized as a sequence of 4 fully-connected ResNet blocks, each block containing 2 fully-connected layers with size 512 and LeakyRelu activation. The encoder reduces the dimensionality to 10. We minimize the loss using ADAM with default parameters, a batch size of 1024, and a decreasing learning rate starting at 0.001. We only give you already trained generative models because creating such VAEs is beyond the scope of this course.\n",
    "\n",
    "To ensure that the prior is well described by a unit variance Gaussian, we augment the forward model by a normalizing flow, using RealNVP. The normalizing flow is trained to map the latent space distribution of the VAE to a standard normal distribution.\n",
    "\n",
    "A more detailed procedure is described below.\n",
    "![alt text](pic1.png \"Title\")\n",
    "We train a vanilla VAE with Gaussian likelihood on uncorrupted training data.\n",
    "![alt text](pic2.png \"Title\")\n",
    "Then, in a second step a bijective normalizing flow is fitted to map the distribution of the encoded training data to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8puPFE90P0aD"
   },
   "outputs": [],
   "source": [
    "# pathes to trained generative models\n",
    "generator_path   = os.path.join(PROJECT_PATH,'modules/decoder1/decoder')\n",
    "encoder_path     = os.path.join(PROJECT_PATH,'modules/encoder1/encoder')\n",
    "nvp_func_path    = os.path.join(PROJECT_PATH,'modules/nvp1/')\n",
    "minima_path      = os.path.join(PROJECT_PATH,'minima/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of VAE and normalizing flow, we can do the posterior analysis in the latent space. We choose the dimension of the latent space to be 10 (set by \"hidden_size\" earlier). First, write a function for the log posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd_pass(generator,nvp,z,mask):\n",
    "    \"\"\"\n",
    "    a pass through the forward model (nvp+VAE generator)\n",
    "    \"\"\" \n",
    "\n",
    "    fwd_z           = nvp({'z_sample':np.zeros((1,hidden_size)),'sample_size':1, 'u_sample':z},as_dict=True)['fwd_pass']\n",
    "\n",
    "    gen_z           = tf.boolean_mask(tf.reshape(generator(fwd_z),[data_size,data_dim,n_channels]),mask, axis=1)\n",
    "\n",
    "    return gen_z\n",
    "\n",
    "\n",
    "def get_likelihood(generator,nvp,z,sigma,mask):\n",
    "    \"\"\"\n",
    "    compute the likelihood for a given latent space vector\n",
    "    \"\"\"\n",
    "    gen_z           = fwd_pass(generator,nvp,z,mask)\n",
    "\n",
    "    sigma           = tf.boolean_mask(sigma,mask, axis=1)\n",
    "\n",
    "    likelihood      = tfd.Independent(tfd.MultivariateNormalDiag(loc=gen_z,scale_diag=sigma))\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "def get_prior():\n",
    "    \"\"\"\n",
    "    return the Gaussian prior\n",
    "    \"\"\"\n",
    "\n",
    "    return tfd.MultivariateNormalDiag(tf.zeros([data_size,hidden_size]), scale_identity_multiplier=1.0, name ='prior')\n",
    "\n",
    "def get_log_posterior(z,x,generator,nvp,sigma,mask, beta):\n",
    "    \"\"\"\n",
    "    returns the negative log posterior (or rather joint dsitribution) - log(p(x,z)), where x is the corrupted input image \n",
    "    \"\"\"\n",
    "\n",
    "    likelihood      = get_likelihood(generator,nvp,z,sigma,mask)\n",
    "\n",
    "    prior           = get_prior()\n",
    "\n",
    "    masked_x        = tf.boolean_mask(x,mask, axis=1)\n",
    "\n",
    "    log_posterior   = prior.log_prob(z)+likelihood.log_prob(masked_x)*beta\n",
    "\n",
    "    return log_posterior\n",
    "\n",
    "\n",
    "def get_recon(generator,nvp, z,sigma,mask):\n",
    "    \"\"\"\n",
    "    forward models a given point in latent space to data space\n",
    "    \"\"\"\n",
    "\n",
    "    prob = get_likelihood(generator,nvp, z,sigma,mask)\n",
    "\n",
    "    recon= prob.mean()\n",
    "\n",
    "    return recon\n",
    "\n",
    "def get_hessian(func, z):\n",
    "    \"\"\"\n",
    "    computes the hessian of function func at point z\n",
    "    \"\"\"\n",
    "\n",
    "    hess             = tf.hessians(func,z)\n",
    "    hess             = tf.gather(hess, 0)\n",
    "\n",
    "    return(tf.reduce_sum(hess, axis = 2 ))\n",
    "\n",
    "\n",
    "def get_GN_hessian(generator,nvp,z,mask,sigma):\n",
    "    '''\n",
    "    computes the Gauss Newton approximation to the Hessian\n",
    "    '''\n",
    "\n",
    "    gen_z            = fwd_pass(generator,nvp,z,mask)\n",
    "\n",
    "    sigma            = tf.boolean_mask(sigma,mask, axis=1)\n",
    "\n",
    "    grad_g           = tf.gather(tf.gradients(gen_z/(sigma),z),0)\n",
    "\n",
    "    grad_g2          = tf.einsum('ij,ik->ijk',grad_g,grad_g)\n",
    "\n",
    "    one              = tf.linalg.eye(hidden_size, batch_shape=[data_size],dtype=tf.float32)\n",
    "\n",
    "    hess_GN          = one+grad_g2\n",
    "\n",
    "    return hess_GN\n",
    "  \n",
    "def compute_covariance(hessian):\n",
    "    \"\"\"\n",
    "    computes the covariance by inverting the hessian (symmetrization for numerical stability)\n",
    "    \"\"\"\n",
    "\n",
    "    cov = tf.linalg.inv(hessian)\n",
    "\n",
    "    cov = (cov+tf.linalg.transpose(cov))*0.5\n",
    "\n",
    "    return cov\n",
    "\n",
    "def get_random_start_values(num, my_sess):\n",
    "    \"\"\"\n",
    "    returns random starting values drawn from the prior\n",
    "    \"\"\"\n",
    "    result=[]\n",
    "    for ii in range(num):\n",
    "        result.append(my_sess.run(get_prior().sample()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define variables and functions needed to perform the posterior analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#### Variables ####\n",
    "#noise level \n",
    "sigma_corr  = tf.placeholder_with_default(np.ones([data_size,data_dim,n_channels], dtype='float32')*sigma_n,shape=[data_size,data_dim,n_channels])\n",
    "#mask\n",
    "mask        = tf.placeholder_with_default(np.ones([data_dim], dtype='float32'),shape=[data_dim])\n",
    "#corrupted input data\n",
    "input_data  = tf.placeholder(shape=[data_size,data_dim,n_channels], dtype=tf.float32)\n",
    "#temperature used for annealed minimzation\n",
    "inverse_T   = tf.placeholder_with_default(1., shape=[])\n",
    "#learning rate\n",
    "lr          = tf.placeholder_with_default(0.001,shape=[])\n",
    "\n",
    "#### Modules ####\n",
    "generator   = hub.Module(generator_path, trainable=False)\n",
    "nvp_funcs   = hub.Module(nvp_func_path, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### for Maximum a Posteriori estimate ####\n",
    "MAP_ini     = tf.placeholder_with_default(tf.zeros([data_size,hidden_size]),shape=[data_size,hidden_size])\n",
    "MAP         = tf.Variable(MAP_ini)\n",
    "MAP_reset   = tf.stop_gradient(MAP.assign(MAP_ini))\n",
    "\n",
    "nlPost_MAP  = get_log_posterior(MAP, input_data, generator,nvp_funcs, sigma_corr,mask, inverse_T)\n",
    "loss_MAP    = -tf.reduce_mean(nlPost_MAP)\n",
    "\n",
    "optimizer   = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "opt_op_MAP  = optimizer.minimize(loss_MAP, var_list=[MAP])\n",
    "\n",
    "recon_MAP   = get_recon(generator,nvp_funcs, MAP,sigma_corr,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### for Laplace approximation around a point ####\n",
    "hessian     = get_hessian(-nlPost_MAP,MAP)\n",
    "GN_hessian  = get_GN_hessian(generator,nvp_funcs,MAP,mask,sigma_corr)\n",
    "ini_val  = np.ones((data_size,(hidden_size *(hidden_size +1)) // 2),dtype=np.float32)\n",
    "with tf.variable_scope(\"Laplace_Posterior\",reuse=tf.AUTO_REUSE):\n",
    "    mu_new      = tf.Variable(np.ones((data_size,hidden_size),dtype=np.float32), dtype=np.float32)\n",
    "    sigma_new_t = ini_val\n",
    "    sigma_new_t2= tf.Variable(tfd.matrix_diag_transform(tfd.fill_triangular(sigma_new_t), transform=tf.nn.softplus),dtype=tf.float32)\n",
    "\n",
    "approx_posterior_laplace = tfd.MultivariateNormalTriL(loc=mu_new,scale_tril=sigma_new_t2)\n",
    "\n",
    "update_mu          = mu_new.assign(MAP)\n",
    "covariance         = compute_covariance(hessian)\n",
    "variance           = tf.linalg.diag_part(covariance)[0]\n",
    "update_TriL        = sigma_new_t2.assign(tf.linalg.cholesky(covariance))\n",
    "\n",
    "posterior_sample   = approx_posterior_laplace.sample()\n",
    "\n",
    "recon              = get_recon(generator,nvp_funcs, posterior_sample ,sigma_corr,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for ELBO\n",
    "\n",
    "########## Syochastic VI estimates by minimizing the elbo #############\n",
    "\n",
    "def get_neg_elbo(x,approx_posterior, generator, nvp, sigma, mask):\n",
    "    \"\"\"\n",
    "    computes the negative evidence lower bound (ELBO)\n",
    "    \"\"\"\n",
    "\n",
    "    prior          = get_prior()\n",
    "    kl_divergence  = tfd.kl_divergence(approx_posterior, prior) \n",
    "\n",
    "    z_sample       = tf.reshape(approx_posterior.sample(sample_size),[-1,hidden_size])\n",
    "\n",
    "    fwd_z          = nvp({'z_sample':np.zeros((1,hidden_size)),'sample_size':1, 'u_sample':z_sample},as_dict=True)['fwd_pass']\n",
    "    gen_z          = tf.boolean_mask(tf.reshape(generator(fwd_z),[sample_size,data_size,data_dim,n_channels]),mask, axis=2)\n",
    "    sig            = tf.boolean_mask(sigma,mask, axis=1)\n",
    "\n",
    "    likelihood     = tfd.Independent(tfd.MultivariateNormalDiag(loc=gen_z,scale_diag=sig))\n",
    "\n",
    "    masked_x       = tf.boolean_mask(x, mask, axis=1)\n",
    "    like_prob      = likelihood.log_prob(tf.expand_dims(masked_x,0))\n",
    "\n",
    "    elbo           = tf.reduce_mean(like_prob)- kl_divergence\n",
    "\n",
    "    return -elbo\n",
    "\n",
    "def minimize_neg_elbo(x,custom_mask,noise,my_sess,full_rank=False):\n",
    "    \"\"\"\n",
    "    function to run a ELBO minimzation for stochastic VI.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if full_rank:\n",
    "        sets = zip([1e-3,1e-4,1e-6],[2000,500,300])\n",
    "    else:\n",
    "        sets = zip([1e-3,1e-4,1e-5],[2000,500,300])\n",
    "\n",
    "    elbo_loss = []\n",
    "    start = time.time()\n",
    "    for lrate, numiter in sets:\n",
    "        print('lrate', lrate)\n",
    "        for jj in range(numiter):\n",
    "            if full_rank:\n",
    "                _, ll = my_sess.run([opt_op_elbo_fr,neg_elbo_fr],feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise, lr: lrate})\n",
    "            else:\n",
    "                _, ll = my_sess.run([opt_op_elbo,neg_elbo],feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise, lr: lrate})\n",
    "            elbo_loss.append(ll)\n",
    "            if jj%1000==0:\n",
    "                print('iter', jj, 'loss', ll)\n",
    "                #if full_rank:\n",
    "                #  print(my_sess.run([mu_vi,post_vi.covariance()],feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise, lr: lrate}))\n",
    "    end= time.time()\n",
    "    print('time taken',end-start)\n",
    "    loss    = ll\n",
    "    plt.figure()\n",
    "    plt.plot(elbo_loss)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.show()\n",
    "\n",
    "    return True\n",
    "  \n",
    "def add_small_offset(x):\n",
    "    \"\"\"offset added to the diagonal of the covariance, needed to make SVI stable\"\"\"\n",
    "    return tf.add(x,1e-8)\n",
    "\n",
    "#### mean field VI ####\n",
    "with tf.variable_scope(\"posterior_elbo_mean\",reuse=tf.AUTO_REUSE):\n",
    "    mu_elbo      = tf.Variable(np.zeros((data_size,hidden_size)), dtype=np.float32)\n",
    "    sigma_elbo   = tf.Variable(np.ones((data_size,hidden_size)), dtype=np.float32)\n",
    "\n",
    "update_mu_elbo = tf.stop_gradient(mu_elbo.assign(MAP))\n",
    "\n",
    "approx_posterior_elbo = tfd.MultivariateNormalDiag(loc=mu_elbo,scale_diag=sigma_elbo, name='approxposterior_elbo')\n",
    "\n",
    "neg_elbo       = get_neg_elbo(input_data, approx_posterior_elbo, generator, nvp_funcs, sigma_corr, mask)\n",
    "opt_op_elbo    = optimizer.minimize(neg_elbo,var_list=[mu_elbo,sigma_elbo])\n",
    "\n",
    "nlPost_elbo    = -tf.reduce_mean(get_log_posterior(mu_elbo, input_data, generator,nvp_funcs, sigma_corr,mask, 1))\n",
    "#######################\n",
    "\n",
    "\n",
    "### full rank VI ###\n",
    "ini_val1     = np.zeros((data_size,(hidden_size *(hidden_size +1)) // 2),dtype=np.float32)\n",
    "with tf.variable_scope(\"posterior_elbo_fullrank\",reuse=tf.AUTO_REUSE):\n",
    "    mu_vi      = tf.Variable(np.ones((data_size,hidden_size)), dtype=np.float32)\n",
    "    sigma_vi   = tf.Variable(ini_val1)\n",
    "sigma_vit    = tfd.matrix_diag_transform(tfd.fill_triangular(sigma_vi), transform=tf.nn.softplus)\n",
    "sigma_vit    = tfd.matrix_diag_transform(sigma_vit, transform=add_small_offset)\n",
    "post_vi      = tfd.MultivariateNormalTriL(loc=mu_vi,scale_tril=sigma_vit)\n",
    "\n",
    "neg_elbo_fr    = get_neg_elbo(input_data, post_vi, generator, nvp_funcs, sigma_corr, mask)\n",
    "opt_op_elbo_fr = optimizer.minimize(neg_elbo_fr,var_list=[mu_vi,sigma_vi])\n",
    "\n",
    "hess_fr        = tf.linalg.inv(post_vi.covariance())\n",
    "nlPost_vi      = -tf.reduce_mean(get_log_posterior(mu_vi, input_data, generator,nvp_funcs, sigma_corr,mask, 1))\n",
    "\n",
    "recon_fr       = get_recon(generator,nvp_funcs, mu_vi ,sigma_corr,mask)\n",
    "recon_elbo     = get_recon(generator,nvp_funcs, mu_elbo ,sigma_corr,mask)\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### for Gaussian mixture model fit around minima #####\n",
    "ini_val2    = np.ones((data_size,num_comp,(hidden_size *(hidden_size +1)) // 2),dtype=np.float32)\n",
    "with tf.variable_scope(\"corrupted/gmm\",reuse=tf.AUTO_REUSE):\n",
    "    mu_gmm      = tf.Variable(np.ones((data_size,num_comp,hidden_size)), dtype=np.float32)\n",
    "    sigma_gmm   = tf.Variable(tfd.fill_triangular(ini_val2))\n",
    "    w_gmm       = tf.Variable(np.ones((num_comp))/num_comp, dtype=np.float32)\n",
    "\n",
    "sigma_gmmt    = tfd.matrix_diag_transform(sigma_gmm, transform=tf.nn.softplus)\n",
    "w_positive    = tf.math.softplus(w_gmm)\n",
    "w_rescaled    = tf.squeeze(w_positive/tf.reduce_sum(w_positive))\n",
    "\n",
    "gmm           = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=w_rescaled),components_distribution=tfd.MultivariateNormalTriL(loc=mu_gmm,scale_tril=sigma_gmmt))\n",
    "\n",
    "mu_ini        = tf.placeholder_with_default(tf.zeros([data_size,num_comp,hidden_size]),shape=[data_size,num_comp,hidden_size])\n",
    "sigma_ini     = tf.placeholder_with_default(tf.ones([data_size,num_comp,hidden_size, hidden_size]),shape=[data_size,num_comp,hidden_size, hidden_size])\n",
    "w_ini         = tf.placeholder_with_default(tf.ones([num_comp])/num_comp,shape=[num_comp])\n",
    "\n",
    "update_w      = tf.stop_gradient(w_gmm.assign(softplus_inverse(w_ini)))\n",
    "update_mugmm  = tf.stop_gradient(mu_gmm.assign(mu_ini))\n",
    "update_TriLgmm= tf.stop_gradient(sigma_gmm.assign(tfd.matrix_diag_transform(sigma_ini, transform=softplus_inverse)))\n",
    "\n",
    "gmm_sample    = gmm.sample()\n",
    "gmm_recon     = get_recon(generator,nvp_funcs, gmm_sample ,sigma_corr,mask)\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run TensorFlow to perform the posterior analysis. Running TensorFlow requires opening up a `Session` which we abbreviate as `sess` for short. All operations are performed in this session by calling the `run` method. First, we initialize the global variables in TensorFlow's computational graph by running the `global_variables_initializer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.random.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use optimization to find all local minima of the negative log posterior. We start from 15 different random positions drawn from the prior and use the ADAM optimizer to descend to the local minima. We stop our search once we find that the minimization procedure repeatedly converges to the same minima, typically after ~10 minimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inits = get_random_start_values(15, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try 3 different learning rate values: 1e-1 with 1100 iterations, 1e-2 with 400 iterations, 1e-3 with 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate_run1 = np.array([1e-1,1e-2,1e-3])\n",
    "numiter_run1 = np.array([1100,400,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for minimizing a posterior loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_posterior(initial_value, x, custom_mask, noise, my_sess, annealing =True):\n",
    "    \"\"\"\n",
    "    function to run several minimizations of the posterior\n",
    "    \"\"\"\n",
    "\n",
    "    ini = np.reshape(initial_value,[data_size,hidden_size])\n",
    "\n",
    "    my_sess.run(MAP_reset,feed_dict={input_data: x, MAP_ini:ini, mask:custom_mask,sigma_corr:noise})\n",
    "\n",
    "    pos_def = False\n",
    "\n",
    "    start = time.time()\n",
    "    posterior_loss = []\n",
    "    for lrate, numiter in zip(lrate_run1, numiter_run1):\n",
    "        for jj in range(numiter):\n",
    "            if annealing and lrate==1e-1:\n",
    "                inv_T= np.round(0.5*np.exp(-(1.-jj/numiter)),decimals=1)\n",
    "            else:\n",
    "                inv_T= 1.\n",
    "            _, ll = my_sess.run([opt_op_MAP,loss_MAP],feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise, lr: lrate, inverse_T:inv_T})\n",
    "            posterior_loss.append(ll)\n",
    "    end = time.time()\n",
    "    z_value = my_sess.run(MAP,feed_dict={input_data: x, mask:custom_mask, sigma_corr:noise})\n",
    "\n",
    "    eig     = my_sess.run(tf.linalg.eigvalsh(hessian),feed_dict={input_data: x, mask:custom_mask,sigma_corr:noise})\n",
    "    if np.all(eig>0.):\n",
    "        pos_def = True\n",
    "\n",
    "    loss    = ll\n",
    "\n",
    "    return z_value, loss, pos_def\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 15 differnt minimizations. See the below code: for each minimization run, we save the latent variable values to `minima`, and the posterior loss values are saved to `min_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minima  =[]\n",
    "min_loss=[]\n",
    "min_var =[]\n",
    "recons  =[]\n",
    "for jj,init in enumerate(inits):\n",
    "    print('progress in %', jj/len(inits)*100)\n",
    "    min_z, min_l, pos_def    = minimize_posterior(init, data,custom_mask,noise,sess)\n",
    "    rec                      = sess.run(recon_MAP, feed_dict={sigma_corr:noise})\n",
    "    var                      = sess.run(variance, feed_dict={input_data: data,mask:custom_mask,sigma_corr:noise})\n",
    "    print('reconstruction run %d (loss %.1f)'%(jj+1, min_l))\n",
    "    plot_image(rec, directory=plot_path, filename='recon_%s_minimum%d'%(label,jj), title='reconstruction with loss %.1f'%min_l)\n",
    "    #If Hessian if positive definite\n",
    "    if pos_def:\n",
    "        minima.append(min_z)\n",
    "        min_loss.append(min_l)\n",
    "        min_var.append(var)\n",
    "        recons.append(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we order the minimization results in the ascending order so that `minima[0]` corresponds to the latent variable values at the deepest minimum of the negative log posterior, which is the MAP solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order    = np.argsort(min_loss)\n",
    "min_loss = np.asarray(min_loss)[order]\n",
    "minima   = np.asarray(minima)[order]\n",
    "min_var  = np.asarray(min_var)[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your minimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([minima, min_loss, min_var,recons],open(minima_path+'minima_%s.pkl'%label,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even if your kernel died due to the memory overflow, you can retrieve your results by running the below cell. (Uncomment the below cell and run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minima, min_loss, min_var, recons = pickle.load(open(minima_path+'minima_%s.pkl'%label,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the minimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_minima(minima, losses, var):\n",
    "    \"\"\"\n",
    "    plot the minima for comparison with their width estimated from the variance at these points -> helps decide whether minima are separate\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title('Minimization result')\n",
    "    plt.plot(np.arange(len(losses)),losses,ls='',marker='o')\n",
    "    plt.xlabel('# iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(plot_path+'minimzation_results_%s.png'%(label),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    colors = matplotlib.colors.Normalize(vmin=min(losses), vmax=max(losses))\n",
    "    cmap   = matplotlib.cm.get_cmap('Spectral')\n",
    "\n",
    "    var = np.squeeze(var)\n",
    "    plt.figure()\n",
    "    plt.title('value of hidden variables at minima')\n",
    "    for ii in range(len(minima)):\n",
    "\n",
    "        yerr_= np.sqrt(var[ii])\n",
    "\n",
    "        plt.errorbar(np.arange(hidden_size),np.squeeze(minima)[ii], marker='o',ls='', c=cmap(colors(losses[ii])), mew=0, yerr=yerr_, label ='%d'%losses[ii])\n",
    "    plt.legend(ncol=4, loc=(1.01,0))\n",
    "    plt.xlabel('# hidden variable')\n",
    "    plt.ylabel('value')\n",
    "    plt.savefig(plot_path+'hidden_values_at_minima_%s.png'%(label),bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_minima(minima, min_loss, min_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set MAP value to the lowest minimum (minima[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowest_minimum = sess.run(MAP_reset, feed_dict={MAP_ini:minima[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the reconstruction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('reconstruction from lowest minimum (MAP solution)')\n",
    "rec     = sess.run(recon_MAP, feed_dict={sigma_corr:noise})\n",
    "plot_image(rec, directory=plot_path, filename='lowest_minimum_%s'%(label), title='reconstruction', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us try stochastic VI (SVI) as a posterior fitting procedure. We use a pre-defined function (see above) 'minimize_neg_elbo' to stochastically minimize the evidence lower bound (ELBO) usig ADAM with decreasing learning rate (the default setting is running with the following learning rate values: 1e-3 with 2000 iterations, 1e-4 with 500 iterations, 1e-5 with 300 iterations - if you have enough memory, you can try more iterations). Here we try both mean field SVI (fitting Gaussians with diagonal) and full rank SVI (full rank covariance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> <i> NOTE: If the kernel dies due to the memory overflow, you can simply re-load optimization results you saved and start from there.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(update_mu_elbo, feed_dict={input_data: data,mask:custom_mask,sigma_corr:noise})\n",
    "print('starting mean field stochastic VI on the posterior')\n",
    "minimize_neg_elbo(data,custom_mask,noise,sess)\n",
    "print('starting full rank stochastic VI on the posterior')\n",
    "minimize_neg_elbo(data,custom_mask,noise,sess, full_rank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the reconstructed images below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('full rank SVI reconstruction')\n",
    "rec     = sess.run(recon_fr, feed_dict={sigma_corr:noise})\n",
    "plot_image(rec, directory=plot_path, filename='full_rank_VI_%s'%(label), title='reconstruction', vmin=0, vmax=1)\n",
    "print('mean field SVI reconstruction')\n",
    "rec     = sess.run(recon_elbo, feed_dict={sigma_corr:noise})\n",
    "plot_image(rec, directory=plot_path, filename='mean_field_VI_%s'%(label), title='reconstruction', vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that full rank SVI results do not look good. That is because we only used a small number of iterations due to the memory limit, suggesting that full rank VI converges to the solution slowly.\n",
    "\n",
    "Now, we can try the EL$_2$O procedure (See slide 33, Lecture 9) - we find the minimum and fit a full rank Gaussian with the Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_laplace_sample(num,map_value,x,mymask,noise,my_sess):\n",
    "    \"\"\"\n",
    "    samples from the Laplace approximation around the deepest minimum\n",
    "    \"\"\"\n",
    "\n",
    "    my_sess.run(MAP_reset,feed_dict={MAP_ini:map_value})\n",
    "    my_sess.run(update_mu)\n",
    "    my_sess.run(update_TriL,feed_dict={input_data: x, mask: mymask, sigma_corr:noise})\n",
    "\n",
    "    samples=[]\n",
    "    for ii in range(num):\n",
    "        my_sess.run(posterior_sample,feed_dict={input_data: x, sigma_corr:noise})\n",
    "        samples.append(my_sess.run(recon,feed_dict={input_data: x, sigma_corr:noise}))\n",
    "\n",
    "    samples=np.asarray(samples)\n",
    "    return samples\n",
    "\n",
    "def get_gmm_sample(num,x,mymask,noise,my_sess):\n",
    "    \"\"\"\n",
    "    samples from the Gaussian mixture posterior fit\n",
    "    \"\"\"\n",
    "\n",
    "    samples=[]\n",
    "    for ii in range(num):\n",
    "        samples.append(my_sess.run(gmm_recon,feed_dict={input_data: x, sigma_corr:noise}))\n",
    "\n",
    "    samples=np.asarray(samples)\n",
    "    return samples\n",
    "\n",
    "def plot_samples(samples, mask, title='samples', filename='samples'):\n",
    "    \"\"\"\n",
    "    plots a compilation of samples\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    for i in range(min(len(samples),16)):\n",
    "        subplot(4,4,i+1)\n",
    "        imshow(np.reshape(samples[i,:],(28,28)),vmin=-0.2,vmax=1.2, cmap='gray')\n",
    "        axis('off')\n",
    "    plt.savefig(plot_path+filename+'.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    if corr_type in ['mask', 'sparse mask', 'noise+mask']:\n",
    "        plt.figure()\n",
    "        plt.title('masked'+title)\n",
    "        for i in range(min(len(samples),16)):\n",
    "            subplot(4,4,i+1)\n",
    "            imshow(np.reshape(samples[i,0,:,0]*mask,(28,28)),vmin=-0.2,vmax=1.2, cmap='gray')\n",
    "            axis('off')     \n",
    "        plt.savefig(plot_path+filename+'masked.png',bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "def get_chi2(sigma,data,mean,masking=True, mask=None,threshold=0.02):\n",
    "    \"\"\"\n",
    "    computes chi2 between data and mean\n",
    "    \"\"\"\n",
    "\n",
    "    if masking:\n",
    "        mask = np.reshape(mask,data.shape)\n",
    "        data = data[np.where(mask==1)]\n",
    "        mean = mean[np.where(mask==1)]\n",
    "        sigma= sigma[np.where(mask==1)]\n",
    "\n",
    "\n",
    "    low = min(sigma.flatten())\n",
    "    high= max(sigma.flatten())\n",
    "\n",
    "    chi2_tot = np.sum((data-mean)**2/sigma**2)\n",
    "    dof_tot  = len(np.squeeze(data))\n",
    "\n",
    "    if corr_type not in ['noise','noise+mask']:\n",
    "        chi2_low = np.sum((data[np.where(data<=threshold)]-mean[np.where(data<=threshold)])**2/sigma[np.where(data<=threshold)]**2)\n",
    "        dof_low  = len(np.squeeze(data[np.where(data<=threshold)]))\n",
    "        chi2_high= np.sum((data[np.where(data>threshold)]-mean[np.where(data>threshold)])**2/sigma[np.where(data>threshold)]**2)\n",
    "        dof_high = len(np.squeeze(data[np.where(data>threshold)]))\n",
    "    else:\n",
    "        chi2_low = None\n",
    "        dof_low  = None\n",
    "        chi2_high= None\n",
    "        dof_high = None\n",
    "\n",
    "    return chi2_tot, dof_tot, chi2_low, dof_low, chi2_high, dof_high, masking\n",
    "\n",
    "def probe_posterior(minimum, x, noise, mymask, my_sess, filename=label):\n",
    "    \"\"\"\n",
    "    probes the real posterior along each latent space direction around the minimum and compares to posterior fits\n",
    "    \"\"\"\n",
    "\n",
    "    _ = my_sess.run(MAP_reset,feed_dict={input_data: x, MAP_ini:minimum, sigma_corr:noise})\n",
    "    _ = my_sess.run(update_mu,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    _ = my_sess.run(update_TriL,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "    exact_hessian = sess.run(hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    approx_hessian= sess.run(GN_hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    ll0 = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "    mean_vi, sig_vi, ll0_vi = sess.run([mu_elbo,sigma_elbo, nlPost_elbo],feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "\n",
    "    mean_fr, h_fr, ll0_fr   = sess.run([mu_vi,hess_fr, nlPost_vi],feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "\n",
    "    print(mean_vi)\n",
    "    print(minimum)\n",
    "\n",
    "    print(ll0,ll0_vi,ll0_fr)\n",
    "\n",
    "    plt.figure(figsize=(4*3,3))\n",
    "\n",
    "\n",
    "    jj =0\n",
    "    for nn in [5,6,7]:#np.arange(hidden_size):\n",
    "        H    = exact_hessian[0,nn,nn]\n",
    "        Hfr  = h_fr[0,nn,nn]\n",
    "        H_vi = 1./sig_vi[0,nn]**2\n",
    "\n",
    "        losses=[]\n",
    "\n",
    "        print(H_vi,H)\n",
    "\n",
    "        subplot(1,3,jj+1)\n",
    "\n",
    "        title('latent space direction %d'%nn)\n",
    "\n",
    "        Delta   = 0.5\n",
    "        steps   = 2000\n",
    "        steps_fine = 10000\n",
    "        delta_z = np.zeros((steps,hidden_size))\n",
    "\n",
    "        delta_z[:,nn] = (np.arange(steps)-steps//2)*Delta/steps\n",
    "        new_ini       = delta_z+minimum\n",
    "        new_ini_vi    = delta_z+mean_vi\n",
    "        new_ini_fr    = delta_z+mean_fr\n",
    "\n",
    "        full_span     = np.ones((steps_fine,hidden_size))*minimum\n",
    "        full_span_min = min(min(new_ini_vi[:,nn]),min(new_ini[:,nn]),min(new_ini_fr[:,nn]))\n",
    "        full_span_max = max(max(new_ini_vi[:,nn]),max(new_ini[:,nn]),max(new_ini_fr[:,nn]))\n",
    "        full_span_    = np.linspace(full_span_min,full_span_max,steps_fine)\n",
    "        full_span[:,nn]= full_span_\n",
    "\n",
    "        for ii in range(steps_fine):\n",
    "            _ = sess.run(MAP_reset,feed_dict={input_data: x, mask:mymask, MAP_ini:np.expand_dims(full_span[ii],axis=0), sigma_corr:noise})\n",
    "            ll = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "            losses.append(ll)\n",
    "\n",
    "\n",
    "        plt.plot(full_span[:,nn],losses,label='probed posterior', color='crimson',lw=2)\n",
    "        plt.plot(new_ini[:,nn],ll0+H*delta_z[:,nn]**2,label='EL2O estimate (full rank)',lw=2)\n",
    "        plt.plot(new_ini_vi[:,nn],ll0_vi+H_vi*delta_z[:,nn]**2,label='mean field SVI estimate',lw=2)\n",
    "        plt.plot(new_ini_fr[:,nn],ll0_fr+Hfr*delta_z[:,nn]**2,label='full rank SVI estimate',lw=2)\n",
    "\n",
    "        plt.xlabel('z',fontsize=16)\n",
    "        plt.ylim(min(min(losses),ll0_fr),max(max(losses),ll0_fr+0.5))\n",
    "        if jj==0:\n",
    "            plt.ylabel('negative log posterior',fontsize=14)\n",
    "            plt.legend(loc='upper right',fontsize=12,framealpha=0.9)\n",
    "        jj+=1\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.savefig(plot_path+'probing_posterior_%s.pdf'%(filename),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def get_gmm_parameters(minima, x, noise, mymask, offset):\n",
    "    \"\"\"\n",
    "    Gaussian mixture model (could be coded more elegantly with softmax function)\n",
    "    \"\"\"\n",
    "\n",
    "    mu   =[]\n",
    "    w    =[]\n",
    "    sigma=[]\n",
    "    for ii in range(num_comp):\n",
    "\n",
    "        # do Laplace approximation around this minimum\n",
    "        mu+=[minima[ii]]\n",
    "        sess.run(MAP_reset,feed_dict={MAP_ini:minima[ii]})\n",
    "        sigma+=[sess.run(update_TriL,feed_dict={input_data: x, sigma_corr:noise, mask: mymask})]\n",
    "\n",
    "        # correct weighting of different minima according to El20 procedure, with samples at the maxima and well seperated maxima\n",
    "        logdet  = sess.run(tf.linalg.logdet(approx_posterior_laplace.covariance()),feed_dict={input_data: x, sigma_corr:noise, mask: mymask})\n",
    "        logprob = sess.run(nlPost_MAP,feed_dict={input_data: x, sigma_corr:noise, mask: mymask})\n",
    "        w+=[np.exp(0.5*logdet+logprob+offset)]\n",
    "\n",
    "    print('weights of Gaussian mixtures:', w/np.sum(w))\n",
    "    mu     = np.reshape(np.asarray(mu),[1,num_comp,hidden_size])\n",
    "    sigma  = np.reshape(np.asarray(sigma),[1,num_comp,hidden_size,hidden_size])\n",
    "    w      = np.squeeze(np.asarray(w))\n",
    "\n",
    "    return mu, sigma, w\n",
    "\n",
    "def plot_prob_2D_GMM(samples, indices):\n",
    "    '''\n",
    "    makes corner plots of the posterior samples\n",
    "    '''\n",
    "\n",
    "    samples = samples[:,0,:]\n",
    "\n",
    "    samples = np.hstack((np.expand_dims(samples[:,indices[0]],-1),np.expand_dims(samples[:,indices[1]],-1)))\n",
    "\n",
    "    figure=corner.corner(samples)\n",
    "    axes = np.array(figure.axes).reshape((2, 2))\n",
    "\n",
    "    axes[1,0].set_xlabel('latent space variable %d'%indices[0])\n",
    "    axes[1,0].set_ylabel('latent space variable %d'%indices[1])\n",
    "    plt.savefig(plot_path+'posterior_contour_GMM_%s_latent_space_dir_%d_%d.png'%(label,indices[0],indices[1]),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw 10 samples from the deepest minimum using the Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = get_laplace_sample(10,minima[0],data,custom_mask,noise,sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the samples (with and without mask) using a pre-defined function 'plot_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('samples from Laplace approximation around deepest minimum (EL2O)')\n",
    "plot_samples(samples, custom_mask, title='Samples from Laplace approximation', filename='samples_laplace_deepest_minimum_%s'%label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use a mixture of multivariate Gaussians (GMM) as posterior approximation. Under the assumption of well separated mixture components, the EL$_2$O procedure for a GMM involves the following 3 steps: 1) finding all relevant minima in the posterior by optimization, 2) fitting Gaussians around these minima by using the Laplace approximation, 3) improving the probability distribution beyond the Gaussian approximation, if needed, 4) finding the relative weights of these mixture components. See the above pre-defined functions for referenes.\n",
    "\n",
    "Remember that you had 15 different minimization results. How many minima points did it have? \n",
    "![alt text](pic3.png \"Title\")\n",
    "In the above plot, you can clearly see three separate minima: iteration 0-1, 2-7, 8-9. Then, you can choose three different minima: minima[0], minima[2], minima[8]. Now, with such minima identified, we fit Gaussians around them and determine their relative weights. Number of Gaussian components we are going to use is determined by `num_comp` variable you set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here, the separate minima have to be set by hand. Take a look at your optimization results and choose three \n",
    "#different minima.\n",
    "# in this run you can choose the following three different minima: minima[0],minima[4],minima[14]\n",
    "mu_, sigma_, w_ = get_gmm_parameters([minima[0],minima[4],minima[14]], data, noise, custom_mask, min_loss[0])\n",
    "_ = sess.run([update_w, update_mugmm,update_TriLgmm], feed_dict={mu_ini:mu_, w_ini:w_, sigma_ini:sigma_ })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple unimodal posterior example, so you should find that the first minima dominates, and the other two are negligible.\n",
    "\n",
    "Finally, take 10 samples from GMM and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = get_gmm_sample(10,data,custom_mask,noise,sess)\n",
    "plot_samples(samples, custom_mask, title='GMM samples', filename='gmm_samples_%s'%label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take 10000 from GMM and plot the posterior in the latent variable space using the corner package. Plot the posterior between latent variable 0 and 1, 1 and 2, and 3 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "more_samples = []\n",
    "for ii in range(10000):\n",
    "    more_samples+=[sess.run(gmm_sample,feed_dict={input_data: data, sigma_corr:noise})]\n",
    "more_samples=np.asarray(more_samples)\n",
    "\n",
    "for indices in [[0,1],[1,2],[3,8]]:\n",
    "    plot_prob_2D_GMM(more_samples, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using a pre-defined function 'probe_posterior', we make a comparison of posterior fits obtained with different fitting procedures to the probed true posterior. We show 3 randomly chosen latent space directions of the 10 dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probe_posterior(minima[0], data, noise, custom_mask, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to observe that EL$_2$O gives high quality posteriors in latent space and in data space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. Using the corrupted data shown below (a broad mask is applied such thata different digits like 4s and 9s are compatible with the data - so you will be able to see a multimodal posterior!), perform the posterior analysis. </i></span> <br>\n",
    "    \n",
    "<span style=\"color:blue\"> <i>(1) When you run optimization to find all local minima of the log posterior, use the following learning rate/number of iteration: 1e-1 with 10000 iterations, 1e-2 with 5000 iterations, 1e-3 with 3000 iterations. (it will take quite a bit of time to run this!) Do 10 runs. Make sure to save your results so that you don't need to run this again.</i></span> <br>\n",
    "\n",
    "<span style=\"color:blue\"> <i>(2) Run the full rank SVI with learning rate of 1e-3 with 3500 iterations, 1e-4 with 500 iterations, and 1e-5 with 500 iterations. Skip the mean field SVI. Make a comparison of the negative log posterior at the first two minimum with EL$_2$O vs. full rank SVI. </i></span> <br>\n",
    "\n",
    "<span style=\"color:blue\"> <i>(3) Fit for 5 different GMM components (corresponding to 5 different minima). Show that the first two dominates and the other three have only negligible contribution.</i></span> <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> <i> NOTE: If the kernel dies when running SVI due to the memory overflow, you can simply re-load optimization results you saved and start from there.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim    = 28*28\n",
    "data_size   = 1\n",
    "sigma_n     = 0.1\n",
    "hidden_size = 10\n",
    "n_channels  = 1\n",
    "seed        = 777\n",
    "sample_size = 512\n",
    "\n",
    "# settings for reconstrcution with rectangular mask\n",
    "corr_type   = 'mask'\n",
    "num_mnist   = 6\n",
    "label       = 'solidmask'\n",
    "noise_level = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comp = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create underlying truth\n",
    "truth = x_test[num_mnist:num_mnist+data_size]\n",
    "# create corrupted data\n",
    "data, custom_mask = make_corrupted_data(truth, corr_type=corr_type)\n",
    "# noise\n",
    "noise = get_custom_noise(data.shape, signal_dependent=False, signal=truth)\n",
    "\n",
    "# Make plot\n",
    "print('Truth:')\n",
    "plot_image(truth, directory=plot_path, filename='truth_%s'%label, title='truth')\n",
    "print('Data:')\n",
    "plot_image(data, directory=plot_path, filename='input_data_%s'%label, title='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probe_posterior(minimum, x, noise, mymask, my_sess, filename=label):\n",
    "    \"\"\"\n",
    "    make comparison: EL2O vs. full rank VI\n",
    "    probes the real posterior along each latent space direction around the minimum and compares to posterior fits\n",
    "    \"\"\"\n",
    "\n",
    "    _ = my_sess.run(MAP_reset,feed_dict={input_data: x, MAP_ini:minimum, sigma_corr:noise})\n",
    "    _ = my_sess.run(update_mu,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    _ = my_sess.run(update_TriL,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "    exact_hessian = sess.run(hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    approx_hessian= sess.run(GN_hessian,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "    ll0 = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "\n",
    "    mean_fr, h_fr, ll0_fr   = sess.run([mu_vi,hess_fr, nlPost_vi],feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "\n",
    "    plt.figure(figsize=(4*3,3))\n",
    "\n",
    "\n",
    "    jj =0\n",
    "    for nn in [5,6,7]:#np.arange(hidden_size):\n",
    "        H    = exact_hessian[0,nn,nn]\n",
    "        Hfr  = h_fr[0,nn,nn]\n",
    "\n",
    "        losses=[]\n",
    "\n",
    "        subplot(1,3,jj+1)\n",
    "\n",
    "        title('latent space direction %d'%nn)\n",
    "\n",
    "        Delta   = 0.5\n",
    "        steps   = 2000\n",
    "        steps_fine = 10000\n",
    "        delta_z = np.zeros((steps,hidden_size))\n",
    "\n",
    "        delta_z[:,nn] = (np.arange(steps)-steps//2)*Delta/steps\n",
    "        new_ini       = delta_z+minimum\n",
    "        new_ini_fr    = delta_z+mean_fr\n",
    "\n",
    "        full_span     = np.ones((steps_fine,hidden_size))*minimum\n",
    "        full_span_min = min(min(new_ini_fr[:,nn]),min(new_ini[:,nn]),min(new_ini_fr[:,nn]))\n",
    "        full_span_max = max(max(new_ini_fr[:,nn]),max(new_ini[:,nn]),max(new_ini_fr[:,nn]))\n",
    "        full_span_    = np.linspace(full_span_min,full_span_max,steps_fine)\n",
    "        full_span[:,nn]= full_span_\n",
    "\n",
    "        for ii in range(steps_fine):\n",
    "            _ = sess.run(MAP_reset,feed_dict={input_data: x, mask:mymask, MAP_ini:np.expand_dims(full_span[ii],axis=0), sigma_corr:noise})\n",
    "            ll = sess.run(loss_MAP,feed_dict={input_data: x, mask:mymask, sigma_corr:noise})\n",
    "            losses.append(ll)\n",
    "\n",
    "\n",
    "        plt.plot(full_span[:,nn],losses,label='probed posterior', color='crimson',lw=2)\n",
    "        plt.plot(new_ini[:,nn],ll0+H*delta_z[:,nn]**2,label='EL2O estimate (full rank)',lw=2)\n",
    "        plt.plot(new_ini_fr[:,nn],ll0_fr+Hfr*delta_z[:,nn]**2,label='full rank SVI estimate',lw=2)\n",
    "\n",
    "        plt.xlabel('z',fontsize=16)\n",
    "        plt.ylim(min(min(losses),ll0_fr),max(max(losses),ll0_fr+0.5))\n",
    "        if jj==0:\n",
    "            plt.ylabel('negative log posterior',fontsize=14)\n",
    "            plt.legend(loc='upper right',fontsize=12,framealpha=0.9)\n",
    "        jj+=1\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.savefig(plot_path+'probing_posterior_%s.pdf'%(filename),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Back to Quasar (Continued from HW7 - Problem 2 - Part 7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "wavelength = np.loadtxt(\"HW5_Problem2_wavelength_300.txt\")\n",
    "X = np.loadtxt(\"HW5_Problem2_QSOspectra_300.txt\")\n",
    "ivar = np.loadtxt(\"HW5_Problem2_ivar_flux_300.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The following analysis is based on https://arxiv.org/pdf/1605.04460.pdf.\n",
    "<br><br>\n",
    "In HW7, we reconstruct the QSO spectra from the noisy data. This reconstructed spectra is closer to the true spectra of QSO. Note that in reality, the true spectra can never be directly observed, both due to measurement error and due to absorption by intervening matter along the line of sight. So we wish to perform inference about the true spectra of QSO using a non-parametric technique called <b>Gaussian processes (GP)</b>. We henceforth call the measured spectra as $y(\\lambda)$ and the true spectra as $f(\\lambda)$ (where $\\lambda$ refers to wavelength).\n",
    "<br><br>\n",
    "A gaussian process is fully specified by its first two central moments: a mean function $\\mu(\\lambda)$ and a covariance function $K(\\lambda, \\lambda')$: <br><br>\n",
    "$$ \\mu(\\lambda) = \\mathbb{E}[f(\\lambda)\\ \\vert\\ \\lambda] $$<br>\n",
    "$$ K(\\lambda, \\lambda') = \\mathrm{cov}[f(\\lambda), f(\\lambda')\\ \\vert\\ \\lambda, \\lambda'] $$.\n",
    "<br>\n",
    "In this problem, we can derive the posterior distribution of $f$ conditioned on the observed values of $y$: <br><br>\n",
    "$$ p(f^*\\ \\vert\\ \\lambda^*, \\lambda, y, \\sigma(\\lambda)^2) = \\mathcal{N}(f^*\\ \\vert\\ \\mu_{f|y}(\\lambda^*), K_{f|y}(\\lambda^*, \\lambda^{*,})) $$\n",
    "<br>\n",
    "where $\\mathcal{N}(f\\ \\vert\\ \\mu, K)$ is a multivariate Gaussian given by: <br>\n",
    "$$ \\mathcal{N}(f\\ \\vert\\ \\mu, K) = \\frac{1}{\\sqrt{(2\\pi)^d \\mathrm{det}K}} \\mathrm{exp}\\big(-\\frac{1}{2}(f-\\mu)^TK^{-1}(f-\\mu)\\big) $$<br>\n",
    "where $d$ is the dimension of $f$.\n",
    "<br><br><br><br>\n",
    "In other words, for the QSO $i$, the measured spectra $y$ is $X_{row\\ i}$. Then, we can compute the posterior distribution of $f$ given $X_{row\\ i}$ as:\n",
    "<br><br>\n",
    "$$ \\mu + \\mathcal{N}\\big(f\\ \\big\\vert\\ \\mu_{f|X_{row\\ i}}, K_{f|X_{row\\ i}}\\big) $$\n",
    "<br>\n",
    "where $\\mu$ is given by:\n",
    "<br><br>\n",
    "$ \\mu$ $ =\n",
    "    \\begin{bmatrix}\n",
    "        \\overline{x}_1 & \\overline{x}_2 & \\dots  &  \\overline{x}_{824} \\\\\n",
    "    \\end{bmatrix}.$\n",
    "<br><br>\n",
    "The mean function $\\mu_{f|X_{row\\ i}}$ and the covariance function $K_{f|X_{row\\ i}}$ are defined as:<br><br>\n",
    "$$ \\mu_{f|X_{row\\ i}} = \\mu\\ +\\ K(K + V)^{-1}(X_{row\\ i} - \\mu)  $$<br>\n",
    "$$ K_{f|X_{row\\ i}} = K - K(K + V)^{-1}K$$\n",
    "<br>\n",
    "where $K = \\phi \\phi^T$ (We can use $\\phi$ from Part 7, Problem 2, HW7. $\\phi$ is a matrix of eigenvectors, its dimension is \"nLambda\" x \"nEigvec\"). <br><br>$V$ is a diagonal matrix whose entries are $\\sigma(\\lambda)^2$ e.g. for the QSO $i$, $V$ = np.diag(1/ivar[i,:]).\n",
    "<br><br>\n",
    "Finally, we can plot $f(\\lambda)$ by sampling from $\\mathcal{N}\\big(f\\ \\big\\vert\\ \\mu_{f|X_{row\\ i}}, K_{f|X_{row\\ i}}\\big)$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. For any two spectra, plot $f(\\lambda)$ using Gaussian processes. You can use np.random.multivariate_normal (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.multivariate_normal.html) to sample from a multivariate Gaussian. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ImageCorruptionMNIST-masknoise05_SVI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
